{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T2_pregunta4.3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9KCAG3BHUVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input,Conv2D,Flatten,Dense,MaxPool2D\n",
        "from keras.models import Model\n",
        "from keras.layers import Reshape,Conv2DTranspose,Activation\n",
        "import numpy as np\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from keras.layers import Lambda\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K \n",
        "import matplotlib.pyplot as plt\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train[:,:,:,None] #add channels\n",
        "x_test = x_test[:,:,:,None]\n",
        "img_rows, img_cols,channel = x_train.shape[1:]\n",
        "original_img_size = (img_rows, img_cols,channel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gxy8bliCjIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32') / 255. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8As6w0buzIna",
        "colab_type": "text"
      },
      "source": [
        "**a) En primer lugar deberá definir la arquitectura realizando unos cambios leves a la presentada anteriormente. Comente las diferencias sobre los parámetros obtenidos.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4OnCx8szKFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters = 32 # number of convolutional filters to use\n",
        "num_conv = 3 # convolution kernel size\n",
        "intermediate_dim = 128\n",
        "latent_dim = 10\n",
        "x = Input(shape=original_img_size)\n",
        "conv_1 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(x)\n",
        "conv_2 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(conv_1)\n",
        "conv_3 = Conv2D(filters*2, kernel_size=num_conv, padding='same', activation='relu', strides=2)(conv_2)\n",
        "flat = Flatten()(conv_3)\n",
        "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
        "z_mean = Dense(latent_dim,activation='linear')(hidden)\n",
        "z_log_var = Dense(latent_dim,activation='linear')(hidden)\n",
        "logits_z = Dense(latent_dim,activation='linear')(hidden) #log(p(z))\n",
        "encoder = Model(x, logits_z) # build a model to project inputs on the latent space\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwJYNc9kzVSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shape_before_flattening = K.int_shape(conv_3)[1:] # we instantiate these layers separately to reuse them later\n",
        "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
        "decoder_upsample = Dense(np.prod(shape_before_flattening), activation='relu')\n",
        "decoder_reshape = Reshape(shape_before_flattening)\n",
        "decoder_deconv_1 = Conv2DTranspose(filters,kernel_size=num_conv, padding='same',strides=2,activation='relu')\n",
        "decoder_deconv_2 = Conv2DTranspose(filters,kernel_size=num_conv,padding='same', activation='relu')\n",
        "decoder_mean_squash = Conv2DTranspose(channel, kernel_size=num_conv,padding='same', activation='sigmoid')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFJko2sNzWAu",
        "colab_type": "code",
        "outputId": "6e77daf4-42fd-4402-a32c-a61254b58fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "def sample_gumbel(shape,eps=K.epsilon()):\n",
        "    \"\"\"Inverse Sample function from Gumbel(0, 1)\"\"\"\n",
        "    U = K.random_uniform(shape, 0, 1)\n",
        "    return - K.log( -K.log(U + eps) + eps)\n",
        "\n",
        "def sampling(logits_z):\n",
        "    \"\"\" Perform a Gumbel-Softmax sampling\"\"\"\n",
        "    tau = K.variable(2/3, name=\"temperature\") \n",
        "    z = logits_z + sample_gumbel(K.shape(logits_z)) # logits + gumbel noise\n",
        "    return keras.activations.softmax( z/tau )    \n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))(logits_z)\n",
        "\n",
        "hid_decoded = decoder_hid(z)\n",
        "up_decoded = decoder_upsample(hid_decoded)\n",
        "reshape_decoded =  decoder_reshape(up_decoded)\n",
        "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
        "x_decoded_relu = decoder_deconv_2(deconv_1_decoded)\n",
        "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
        "vae_norm = Model(x, x_decoded_mean_squash) # instantiate VAE model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0806 01:10:23.242040 140327751669632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukC-lViJ5mi8",
        "colab_type": "code",
        "outputId": "a2cdbd0a-cb45-4411-9121-d03770fcf41f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        }
      },
      "source": [
        "# Compute VAE loss\n",
        "choised_loss =  keras.metrics.binary_crossentropy(K.flatten(x),K.flatten(x_decoded_mean_squash))\n",
        "choised_loss =  keras.metrics.mean_squared_error(K.flatten(x),K.flatten(x_decoded_mean_squash))\n",
        "reconstruction_loss = img_rows * img_cols * channel* choised_loss\n",
        "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) #closed form\n",
        "dist =  keras.activations.softmax(logits_z) # =p(z)\n",
        "dist_neg_entropy = K.sum(dist * K.log(dist + K.epsilon()), axis=1)\n",
        "kl_disc_loss =  np.log(latent_dim) + dist_neg_entropy #discrete KL-loss\n",
        "vae_loss = K.mean(reconstruction_loss + kl_disc_loss)\n",
        "vae_norm.add_loss(vae_loss)\n",
        "vae_norm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0806 01:10:28.296645 140327751669632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               1605760   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               1408      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 12544)             1618176   \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         289       \n",
            "=================================================================\n",
            "Total params: 3,282,699\n",
            "Trainable params: 3,282,699\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rQRRvAb4pgE",
        "colab_type": "text"
      },
      "source": [
        "**b) Entrene el VAE categórico de la misma manera que realizó con el VAE tradicional en (e) ¿Nota algún cambio en este paso?.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbv-ZnDK4tUF",
        "colab_type": "code",
        "outputId": "ac031da9-e275-45a2-cce7-93f1f0cd97f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 30\n",
        "vae_norm.compile(optimizer='rmsprop')\n",
        "h = vae_norm.fit(x_train, epochs=epochs, batch_size=batch_size,validation_data=(x_test, None))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0802 15:15:11.585612 140301506402176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0802 15:15:11.829242 140301506402176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 19s 313us/step - loss: 43.8141 - val_loss: 5785789.3080\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 38.2086 - val_loss: 5785515.0320\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 35.7010 - val_loss: 5782965.5608\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 34.0495 - val_loss: 5781536.4800\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 33.0397 - val_loss: 5784029.5888\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 32.2743 - val_loss: 5783655.5648\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 31.6754 - val_loss: 5781435.7768\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 31.2031 - val_loss: 5783203.9144\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 30.7780 - val_loss: 5782170.5864\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 30.4254 - val_loss: 5785808.8576\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 30.1276 - val_loss: 5783609.0320\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 29.8956 - val_loss: 5783863.8480\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 29.6862 - val_loss: 5786105.2240\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 29.4669 - val_loss: 5784411.6056\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 29.2045 - val_loss: 5782596.1168\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 28.9801 - val_loss: 5788558.7344\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 28.7983 - val_loss: 5787164.6504\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 28.6241 - val_loss: 5786364.8152\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 28.4031 - val_loss: 5786245.9576\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 28.2887 - val_loss: 5787941.4320\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 28.1054 - val_loss: 5786239.7752\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 27.9657 - val_loss: 5787690.1808\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 27.8825 - val_loss: 5786296.2416\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 27.7347 - val_loss: 5791124.7352\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 27.6752 - val_loss: 5788415.9296\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 27.6049 - val_loss: 5791234.6072\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 27.4643 - val_loss: 5788133.4456\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 27.3830 - val_loss: 5788319.9872\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 27.2718 - val_loss: 5786906.2248\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 27.1669 - val_loss: 5790206.0624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9a32016f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwD4GjVl7ftf",
        "colab_type": "text"
      },
      "source": [
        "**c) Para ver la efectividad del encoder en lograr capturar las clases es necesario medir una métrica de desempeño, sin embargo, las métricas clásicas como accuracy o f1 score no corresponderían a este caso debido a que las categoría capturada por el encoder no debería estar en el mismo orden de la clase real, ya que fue un entrenamiento no supervisado ¿Cómo se podría cambiar ésto?. Con esto en mente mida alguna métrica de clustering [11] sobre las categorías inferidas por el VAE. Comente.\n",
        "Recuerde que se predice los logits de la probabilidad**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgX955_O7vXx",
        "colab_type": "code",
        "outputId": "e452e27b-40ea-4807-c02a-04e5185f72cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - x.max(axis=-1,keepdims=True) )\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "p_z_train = softmax(encoder.predict(x_train))\n",
        "p_z_test = softmax(encoder.predict(x_test))\n",
        "y_train_pred = p_z_train.argmax(axis=-1)\n",
        "y_test_pred = p_z_test.argmax(axis=-1)\n",
        "\n",
        "\n",
        "print(normalized_mutual_info_score(y_train, y_train_pred))\n",
        "print(normalized_mutual_info_score(y_test, y_test_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2840687496863312\n",
            "0.2933177702860208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6irVTVf73WL",
        "colab_type": "text"
      },
      "source": [
        "**d) Para entender mejor las categorías inferidas por el VAE genere datos \"activando\" una categoría y luego realizando un forward pass sobre el decoder/generador. Comente cualitativamente.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62jdRnAD724x",
        "colab_type": "code",
        "outputId": "8d0ba9b9-bf90-4007-92d2-5209f56bce59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "decoder_input = Input(shape=(latent_dim,))\n",
        "_hid_decoded = decoder_hid(decoder_input)\n",
        "_up_decoded = decoder_upsample(_hid_decoded)\n",
        "_reshape_decoded = decoder_reshape(_up_decoded)\n",
        "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
        "_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n",
        "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
        "generator = Model(decoder_input, _x_decoded_mean_squash) \n",
        "predictions =np.zeros((img_cols * 1 ,img_cols* latent_dim))\n",
        "\n",
        "for i in range(latent_dim):\n",
        "    activate_aux = np.zeros((1,10))\n",
        "    activate_aux[:,i] = 1 #activate a class\n",
        "    predictions[:,i * img_cols:(i + 1) * img_cols] = np.squeeze(generator.predict(activate_aux))\n",
        "    \n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(predictions, cmap='gnuplot2')\n",
        "pos = np.arange(img_cols/2, img_cols*latent_dim, img_cols)\n",
        "plt.xticks(pos,range(latent_dim))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABaCAYAAACc0dMJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTkKfwJp8NbW",
        "colab_type": "text"
      },
      "source": [
        "**e) Genere algunas imágenes aleatorias, comente cualitativamente con lo obtenido con el VAE tradicional ¿Cuál pareciera ser mejor para generar datos? ¿Por qué?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffM4dS6W8QKY",
        "colab_type": "code",
        "outputId": "5c8c79e7-d9dc-4eab-b8d4-2da859cae427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "decoder_input = Input(shape=(latent_dim,))\n",
        "_hid_decoded = decoder_hid(decoder_input)\n",
        "_up_decoded = decoder_upsample(_hid_decoded)\n",
        "_reshape_decoded = decoder_reshape(_up_decoded)\n",
        "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
        "_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n",
        "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
        "generator = Model(decoder_input, _x_decoded_mean_squash) \n",
        "\n",
        "n = 30  # figure with 15x15 images \n",
        "image_size = img_cols\n",
        "figure = np.zeros((image_size * n, image_size * n))\n",
        "from scipy.stats import norm\n",
        "grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) #metodo de la transformada inversa\n",
        "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "\n",
        "for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "        z_sample = np.array([[xi, yi]])            \n",
        "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
        "        figure[i * image_size: (i + 1) * image_size,\n",
        "               j * image_size: (j + 1) * image_size] = x_decoded[0][:,:,0]\n",
        "        \n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure,cmap='gnuplot2')\n",
        "pos = np.arange(image_size/2,image_size*n,image_size)\n",
        "plt.yticks(pos,np.round(grid_y,1))\n",
        "plt.xticks(pos,np.round(grid_x,1))\n",
        "plt.show()\n",
        "grid = norm.ppf(np.linspace(0.000005, 0.999995, n)) #en los extremos del intervalo de confianza"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9d9c15483037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mz_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         figure[i * image_size: (i + 1) * image_size,\n\u001b[1;32m     22\u001b[0m                j * image_size: (j + 1) * image_size] = x_decoded[0][:,:,0]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_5 to have shape (10,) but got array with shape (2,)"
          ]
        }
      ]
    }
  ]
}