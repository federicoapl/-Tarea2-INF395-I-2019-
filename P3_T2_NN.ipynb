{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "T2.3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUUUfgOTYzhy",
        "colab_type": "text"
      },
      "source": [
        "> a) Visualice los datos ¿Qué es la entrada y qué es la salida? Comente sobre los múltiples significados/sinónimos que puede tener una palabra al ser traducida y cómo propondría arreglar eso. *se espera que pueda implementarlo*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JINnElwTYzhz",
        "colab_type": "text"
      },
      "source": [
        "Vamos a partir trabajando con una sistema de traducción ingles-frances, el primer paso como siempre es cargar los datos y hacer un análisis de estos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ndr64uAYzh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "411e7d2b-1cc7-44d7-84d0-25680ce9e7d7"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,CuDNNGRU\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import CuDNNGRU, TimeDistributed,Dense"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-OE40oyYzh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "#import helper\n",
        "import numpy as np\n",
        "#import project_tests as tests\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7ZVZzrLYzh8",
        "colab_type": "code",
        "outputId": "baeb28e8-0a5b-41fa-d5ab-441ec0a7635c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"fra.txt\", sep=\"\\t\", names=[\"Source\",\"Target\"])\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Qui ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Source    Target\n",
              "0    Go.      Va !\n",
              "1    Hi.   Salut !\n",
              "2   Run!   Cours !\n",
              "3   Run!  Courez !\n",
              "4   Who?     Qui ?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkE6QezUYziA",
        "colab_type": "text"
      },
      "source": [
        "La entrada (Source) en este caso son frases en ingles, las primeras que se ven son frases de una sola palabra. La salida (Target) es su traducción al frances, no es una traducción literal, en su lugar se trata de la interpretación o simil de la frase en frances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA-lIMPDYziB",
        "colab_type": "code",
        "outputId": "8bcaa853-34cc-4e39-a696-9c77d0080c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "df[17:19]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>Bonjour !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Source     Target\n",
              "17  Hello!  Bonjour !\n",
              "18  Hello!    Salut !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMHQ5nxUYziE",
        "colab_type": "code",
        "outputId": "116b15f0-ad16-4dd4-cad7-7cc32255607e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df.describe"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of                                                    Source                                             Target\n",
              "0                                                     Go.                                               Va !\n",
              "1                                                     Hi.                                            Salut !\n",
              "2                                                    Run!                                            Cours !\n",
              "3                                                    Run!                                           Courez !\n",
              "4                                                    Who?                                              Qui ?\n",
              "5                                                    Wow!                                         Ça alors !\n",
              "6                                                   Fire!                                           Au feu !\n",
              "7                                                   Help!                                         À l'aide !\n",
              "8                                                   Jump.                                             Saute.\n",
              "9                                                   Stop!                                        Ça suffit !\n",
              "10                                                  Stop!                                             Stop !\n",
              "11                                                  Stop!                                       Arrête-toi !\n",
              "12                                                  Wait!                                          Attends !\n",
              "13                                                  Wait!                                         Attendez !\n",
              "14                                                 Go on.                                          Poursuis.\n",
              "15                                                 Go on.                                         Continuez.\n",
              "16                                                 Go on.                                        Poursuivez.\n",
              "17                                                 Hello!                                          Bonjour !\n",
              "18                                                 Hello!                                            Salut !\n",
              "19                                                 I see.                                      Je comprends.\n",
              "20                                                 I try.                                          J'essaye.\n",
              "21                                                 I won!                                       J'ai gagné !\n",
              "22                                                 I won!                                  Je l'ai emporté !\n",
              "23                                                 I won.                                        J’ai gagné.\n",
              "24                                                 Oh no!                                           Oh non !\n",
              "25                                                Attack!                                          Attaque !\n",
              "26                                                Attack!                                         Attaquez !\n",
              "27                                                Cheers!                                            Santé !\n",
              "28                                                Cheers!                                    À votre santé !\n",
              "29                                                Cheers!                                            Merci !\n",
              "...                                                   ...                                                ...\n",
              "167100  A man who has never gone to school may steal f...  Un homme qui n'a jamais été à l'école peut vol...\n",
              "167101  One way to lower the number of errors in the T...  Un moyen de diminuer le nombre d’erreurs dans ...\n",
              "167102  What is old age? First you forget names, then ...  Qu'est l'âge ? D'abord on oublie les noms, et ...\n",
              "167103  What is old age? First you forget names, then ...  Ce qu'est l'âge ? D'abord on oublie les noms, ...\n",
              "167104  He and I have a near-telepathic understanding ...  Lui et moi avons une compréhension quasi-télép...\n",
              "167105  Although rainforests make up only two percent ...  Bien que les forêts tropicales ne couvrent que...\n",
              "167106  She has a boyfriend she's been going out with ...  Elle a un copain avec qui elle sort depuis le ...\n",
              "167107  If you translate from your second language int...  Si vous traduisez de votre seconde langue dans...\n",
              "167108  I love trying out new things, so I always buy ...  J'adore essayer de nouvelles choses, alors j'a...\n",
              "167109  A good theory is characterized by the fact tha...  Une bonne théorie se caractérise par le fait d...\n",
              "167110  The more time you spend speaking a foreign lan...  Plus l'on passe de temps à parler une langue é...\n",
              "167111  The enquiry concluded that, despite his denial...  L'enquête conclut qu'en dépit de ses dénégatio...\n",
              "167112  The Tatoeba Project, which can be found online...  Le projet Tatoeba, que l'on peut trouver en li...\n",
              "167113  You may not learn to speak as well as a native...  Peut-être n'apprendrez-vous pas à parler comme...\n",
              "167114  And the good news is that today the economy is...  Et la bonne nouvelle est qu'aujourd'hui l'écon...\n",
              "167115  E-cigarettes are being promoted as a healthy a...  La cigarette électronique est mise en avant co...\n",
              "167116  It's still too hard to find a job. And even if...  C'est encore trop difficile de trouver un empl...\n",
              "167117  As you contribute more sentences to the Tatoeb...  Au fur et à mesure que vous ajoutez davantage ...\n",
              "167118  Even at the end of the nineteenth century, sai...  Même à la fin du dix-neuvième siècle, les mari...\n",
              "167119  Five tremors in excess of magnitude 5.0 on the...  Cinq secousses dépassant la magnitude cinq sur...\n",
              "167120  No matter how much you try to convince people ...  Peu importe le temps que tu passeras à essayer...\n",
              "167121  We need to uphold laws against discrimination ...  Nous devons faire respecter les lois contre la...\n",
              "167122  A child who is a native speaker usually knows ...  Un enfant qui est un locuteur natif connaît ha...\n",
              "167123  There are four main causes of alcohol-related ...  Il y a quatre causes principales de décès liés...\n",
              "167124  Top-down economics never works, said Obama. \"T...  « L'économie en partant du haut vers le bas, ç...\n",
              "167125  A carbon footprint is the amount of carbon dio...  Une empreinte carbone est la somme de pollutio...\n",
              "167126  Death is something that we're often discourage...  La mort est une chose qu'on nous décourage sou...\n",
              "167127  Since there are usually multiple websites on a...  Puisqu'il y a de multiples sites web sur chaqu...\n",
              "167128  If someone who doesn't know your background sa...  Si quelqu'un qui ne connaît pas vos antécédent...\n",
              "167129  It may be impossible to get a completely error...  Il est peut-être impossible d'obtenir un Corpu...\n",
              "\n",
              "[167130 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDFtVwaaYziH",
        "colab_type": "code",
        "outputId": "640a3cec-c360-4f67-e7e6-6dec1fd72341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Source    167130\n",
              "Target    167130\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwYtB-GBYziL",
        "colab_type": "code",
        "outputId": "94b81815-2b7a-4797-d3fd-73699ab54ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "df[0:100000].groupby(['Source']).count().plot(kind='bar')\n",
        "cur_axes = plt.gca()\n",
        "cur_axes.axes.get_xaxis().set_visible(False)\n",
        "plt.ylim(0,10)\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADxCAYAAADSguz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDgK1XG6YziO",
        "colab_type": "code",
        "outputId": "d964d204-d55e-48a1-9430-9576925c6b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "df[0:10].groupby(['Source']).count().plot(kind='bar')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7b9db71eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEcCAYAAADA5t+tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nj4QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEEEDkLCYziR",
        "colab_type": "text"
      },
      "source": [
        "Para hacer una limpieza de este tipo de casos se debería definir un criterio claro sobre intepretaciones, por ejemplo Bonjour es buenos días y Salut es saludos, es esta forma sería mejor que se usase good morning y hello respectivamente. Se necesita un contexto para poder traducir correctamente esta simple frase de una palabra.\n",
        "La otra alternativa es simplemente elegir un representante por frase, quizas la raiz de todas las conjugaciónes.\n",
        "Según los resultados de las redes neuronales se volverá a este punto para evaluar los efecto de los cambios hechos.\n",
        "Alternativamente se pueden entregar más de una opción de traducción, por ejemplo la segunda y/o tercera traducción más probable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoq0yV88YziS",
        "colab_type": "text"
      },
      "source": [
        "El gráfico sirve para ver que la repetición de palabras en frances es más comun de lo que uno creería previamente, con conocimiento previo esto se puede corroborar ya que el frances diferencia más claramente cuando el sujeto es hombre o mujer. El ejemplo de Run sirve para ejemplificar eso, las dos acepciones se refieren a cuando es un hombre y cuando se trata de una mujer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psvdqEaJYziT",
        "colab_type": "code",
        "outputId": "4ac321dd-5f6e-4225-a54f-24bf34d66b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "ingles_contadorr = collections.Counter([word for sentence in df['Source'] for word in sentence.split()])\n",
        "frances_contador = collections.Counter([word for sentence in df['Target'] for word in sentence.split()])\n",
        "print('{} Palabras en Ingles.'.format(len([word for sentence in df['Source'] for word in sentence.split()])))\n",
        "print('{} Palabras únicas en Ingles.'.format(len(ingles_contadorr)))\n",
        "print('10 Palabras más comunes del dataset de Ingles:')\n",
        "print('\"' + '\" \"'.join(list(zip(*ingles_contadorr.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} Palabras en Frances.'.format(len([word for sentence in df['Target'] for word in sentence.split()])))\n",
        "print('{} alabras únicas en Frances.'.format(len(frances_contador)))\n",
        "print('10  Palabras más comunes del dataset de Frances:')\n",
        "print('\"' + '\" \"'.join(list(zip(*frances_contador.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1028095 Palabras en Ingles.\n",
            "26936 Palabras únicas en Ingles.\n",
            "10 Palabras más comunes del dataset de Ingles:\n",
            "\"I\" \"to\" \"you\" \"the\" \"a\" \"is\" \"Tom\" \"of\" \"in\" \"He\"\n",
            "\n",
            "1118669 Palabras en Frances.\n",
            "44228 alabras únicas en Frances.\n",
            "10  Palabras más comunes del dataset de Frances:\n",
            "\"de\" \"Je\" \"?\" \"pas\" \"que\" \"à\" \"ne\" \"la\" \"le\" \"Il\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMjXhl4YziW",
        "colab_type": "text"
      },
      "source": [
        "> b) Realice un pre-procesamiento a los textos como se acostumbra para eliminar símbolos inecesarios u otras cosas que estime conveniente, comente sobre la importancia de éste paso. Además de ésto deberá agregar un símbolo al final de la sentencia *target* para indicar un \"alto\" cuando la red neuronal necesite aprender a generar una sentencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODYO-0kmYziX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "table = str.maketrans('', '', string.punctuation) \n",
        "def clean_text(text, where=None):\n",
        "    \"\"\" OJO: Sin eliminar el significado de las palabras.\"\"\"\n",
        "    text = text.lower()\n",
        "    tokenize_text = text.split()\n",
        "    tokenize_text = [word.translate(table) for word in tokenize_text]#eliminar puntuacion\n",
        "    tokenize_text = [word for word in tokenize_text if word.isalpha()] #remove numbers\n",
        "    if where ==\"target\":\n",
        "        tokenize_text = tokenize_text + [\"#end\"] \n",
        "    return tokenize_text\n",
        "texts_input = list(df['Source'].apply(clean_text))\n",
        "texts_output = list(df['Target'].apply(clean_text, where='target'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcu303ytYzib",
        "colab_type": "text"
      },
      "source": [
        "La primera parte, table, generea un mapeo de diccionario de cada caracter en string punctuation a nada. El resto queda bastante claro.\n",
        "\n",
        "Este paso e simportante pues nos entrega un formato más comprensible para la maquina, se trata de una lista ordenada de strings, la abstracción del lenguaje no es total ya que las palabras siguen teniendo tiempos verbales lo que es importante para traducir textos y entender su contexto, pero por otro lado los numero se pueden traducir textualmente, y con la puntuación se puede hacer algo similiar, con mayor o menor efectividad dependiendo del tipo, los signos de exclamación y pregunta mantienen su ubicación sin mayor drama, pero las comas no tienen un análogo tan directo.\n",
        "Se mantuvo la señal \"#end\" para marcar el final de las frase en la nueva lista de 'Target'. No hay razón para cambiarla, ya que no se van a encontrar palabras en frances que usen # ni end. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_2dyZz3Yzic",
        "colab_type": "code",
        "outputId": "3f639919-ba36-4bd2-a51c-cb18c48a0156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "texts_output"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['va', '#end'],\n",
              " ['salut', '#end'],\n",
              " ['cours', '#end'],\n",
              " ['courez', '#end'],\n",
              " ['qui', '#end'],\n",
              " ['ça', 'alors', '#end'],\n",
              " ['au', 'feu', '#end'],\n",
              " ['à', 'laide', '#end'],\n",
              " ['saute', '#end'],\n",
              " ['ça', 'suffit', '#end'],\n",
              " ['stop', '#end'],\n",
              " ['arrêtetoi', '#end'],\n",
              " ['attends', '#end'],\n",
              " ['attendez', '#end'],\n",
              " ['poursuis', '#end'],\n",
              " ['continuez', '#end'],\n",
              " ['poursuivez', '#end'],\n",
              " ['bonjour', '#end'],\n",
              " ['salut', '#end'],\n",
              " ['je', 'comprends', '#end'],\n",
              " ['jessaye', '#end'],\n",
              " ['jai', 'gagné', '#end'],\n",
              " ['je', 'lai', 'emporté', '#end'],\n",
              " ['gagné', '#end'],\n",
              " ['oh', 'non', '#end'],\n",
              " ['attaque', '#end'],\n",
              " ['attaquez', '#end'],\n",
              " ['santé', '#end'],\n",
              " ['à', 'votre', 'santé', '#end'],\n",
              " ['merci', '#end'],\n",
              " ['tchintchin', '#end'],\n",
              " ['lèvetoi', '#end'],\n",
              " ['va', 'maintenant', '#end'],\n",
              " ['allezy', 'maintenant', '#end'],\n",
              " ['vasy', 'maintenant', '#end'],\n",
              " ['jai', 'pigé', '#end'],\n",
              " ['compris', '#end'],\n",
              " ['pigé', '#end'],\n",
              " ['compris', '#end'],\n",
              " ['tas', 'capté', '#end'],\n",
              " ['monte', '#end'],\n",
              " ['montez', '#end'],\n",
              " ['serremoi', 'dans', 'tes', 'bras', '#end'],\n",
              " ['serrezmoi', 'dans', 'vos', 'bras', '#end'],\n",
              " ['je', 'suis', 'tombée', '#end'],\n",
              " ['je', 'suis', 'tombé', '#end'],\n",
              " ['je', 'sais', '#end'],\n",
              " ['je', 'suis', 'parti', '#end'],\n",
              " ['je', 'suis', 'partie', '#end'],\n",
              " ['jai', 'perdu', '#end'],\n",
              " ['payé', '#end'],\n",
              " ['jai', 'ans', '#end'],\n",
              " ['je', 'vais', 'bien', '#end'],\n",
              " ['ça', 'va', '#end'],\n",
              " ['écoutez', '#end'],\n",
              " ['cest', 'pas', 'possible', '#end'],\n",
              " ['impossible', '#end'],\n",
              " ['en', 'aucun', 'cas', '#end'],\n",
              " ['sans', 'façons', '#end'],\n",
              " ['cest', 'hors', 'de', 'question', '#end'],\n",
              " ['il', 'nen', 'est', 'pas', 'question', '#end'],\n",
              " ['cest', 'exclu', '#end'],\n",
              " ['en', 'aucune', 'manière', '#end'],\n",
              " ['hors', 'de', 'question', '#end'],\n",
              " ['vraiment', '#end'],\n",
              " ['vrai', '#end'],\n",
              " ['ah', 'bon', '#end'],\n",
              " ['merci', '#end'],\n",
              " ['on', 'essaye', '#end'],\n",
              " ['nous', 'avons', 'gagné', '#end'],\n",
              " ['nous', 'gagnâmes', '#end'],\n",
              " ['nous', 'lavons', 'emporté', '#end'],\n",
              " ['nous', 'lemportâmes', '#end'],\n",
              " ['demande', 'à', 'tom', '#end'],\n",
              " ['fantastique', '#end'],\n",
              " ['sois', 'calme', '#end'],\n",
              " ['soyez', 'calme', '#end'],\n",
              " ['soyez', 'calmes', '#end'],\n",
              " ['sois', 'détendu', '#end'],\n",
              " ['sois', 'juste', '#end'],\n",
              " ['soyez', 'juste', '#end'],\n",
              " ['soyez', 'justes', '#end'],\n",
              " ['sois', 'équitable', '#end'],\n",
              " ['soyez', 'équitable', '#end'],\n",
              " ['soyez', 'équitables', '#end'],\n",
              " ['sois', 'gentil', '#end'],\n",
              " ['sois', 'gentil', '#end'],\n",
              " ['sois', 'gentille', '#end'],\n",
              " ['soyez', 'gentil', '#end'],\n",
              " ['soyez', 'gentille', '#end'],\n",
              " ['soyez', 'gentils', '#end'],\n",
              " ['soyez', 'gentilles', '#end'],\n",
              " ['dégage', '#end'],\n",
              " ['appellemoi', '#end'],\n",
              " ['appellezmoi', '#end'],\n",
              " ['appellenous', '#end'],\n",
              " ['appeleznous', '#end'],\n",
              " ['entrez', '#end'],\n",
              " ['entre', '#end'],\n",
              " ['entre', '#end'],\n",
              " ['entrez', '#end'],\n",
              " ['allez', '#end'],\n",
              " ['allez', '#end'],\n",
              " ['viens', '#end'],\n",
              " ['venez', '#end'],\n",
              " ['laisse', 'tomber', '#end'],\n",
              " ['laissez', 'tomber', '#end'],\n",
              " ['laissele', 'tomber', '#end'],\n",
              " ['laissezle', 'tomber', '#end'],\n",
              " ['va', 'chercher', 'tom', '#end'],\n",
              " ['sortez', '#end'],\n",
              " ['sors', '#end'],\n",
              " ['sortez', '#end'],\n",
              " ['sors', '#end'],\n",
              " ['cassetoi', '#end'],\n",
              " ['dégage', '#end'],\n",
              " ['pars', '#end'],\n",
              " ['va', 'te', 'faire', 'foutre', '#end'],\n",
              " ['pars', '#end'],\n",
              " ['dégage', '#end'],\n",
              " ['fous', 'le', 'camp', '#end'],\n",
              " ['pars', 'dici', '#end'],\n",
              " ['va', 'ten', '#end'],\n",
              " ['disparais', '#end'],\n",
              " ['allezvous', 'en', '#end'],\n",
              " ['rentrez', 'à', 'la', 'maison', '#end'],\n",
              " ['rentre', 'à', 'la', 'maison', '#end'],\n",
              " ['rentre', 'chez', 'toi', '#end'],\n",
              " ['rentrez', 'chez', 'vous', '#end'],\n",
              " ['va', 'doucement', '#end'],\n",
              " ['allez', 'doucement', '#end'],\n",
              " ['à', 'la', 'revoyure', '#end'],\n",
              " ['attends', 'un', 'peu', '#end'],\n",
              " ['attendez', 'un', 'peu', '#end'],\n",
              " ['tiens', 'bon', '#end'],\n",
              " ['tenez', 'bon', '#end'],\n",
              " ['il', 'laissa', 'tomber', '#end'],\n",
              " ['il', 'a', 'laissé', 'tomber', '#end'],\n",
              " ['il', 'court', '#end'],\n",
              " ['aidemoi', '#end'],\n",
              " ['aidemoi', '#end'],\n",
              " ['aidezmoi', '#end'],\n",
              " ['aideznous', '#end'],\n",
              " ['aidenous', '#end'],\n",
              " ['ne', 'bouge', 'plus', '#end'],\n",
              " ['ne', 'quittez', 'pas', '#end'],\n",
              " ['fais', 'un', 'câlin', 'à', 'tom', '#end'],\n",
              " ['je', 'suis', 'du', 'même', 'avis', '#end'],\n",
              " ['jai', 'pleuré', '#end'],\n",
              " ['je', 'me', 'suis', 'assoupi', '#end'],\n",
              " ['je', 'me', 'suis', 'assoupie', '#end'],\n",
              " ['je', 'conduis', '#end'],\n",
              " ['je', 'fume', '#end'],\n",
              " ['je', 'ronfle', '#end'],\n",
              " ['je', 'pue', '#end'],\n",
              " ['je', 'me', 'suis', 'tenu', 'debout', '#end'],\n",
              " ['je', 'me', 'suis', 'tenue', 'debout', '#end'],\n",
              " ['promis', '#end'],\n",
              " ['juré', '#end'],\n",
              " ['jessayai', '#end'],\n",
              " ['jai', 'essayé', '#end'],\n",
              " ['jai', 'tenté', '#end'],\n",
              " ['fait', 'signe', '#end'],\n",
              " ['jirai', '#end'],\n",
              " ['je', 'suis', 'tom', '#end'],\n",
              " ['je', 'suis', 'gras', '#end'],\n",
              " ['je', 'suis', 'gros', '#end'],\n",
              " ['je', 'suis', 'en', 'forme', '#end'],\n",
              " ['je', 'suis', 'touché', '#end'],\n",
              " ['je', 'suis', 'touchée', '#end'],\n",
              " ['je', 'suis', 'malade', '#end'],\n",
              " ['je', 'suis', 'triste', '#end'],\n",
              " ['je', 'suis', 'timide', '#end'],\n",
              " ['je', 'suis', 'mouillé', '#end'],\n",
              " ['je', 'suis', 'mouillée', '#end'],\n",
              " ['cest', 'bibi', '#end'],\n",
              " ['joignezvous', '#end'],\n",
              " ['joignezvous', 'à', 'nous', '#end'],\n",
              " ['gardele', '#end'],\n",
              " ['gardezle', '#end'],\n",
              " ['embrassemoi', '#end'],\n",
              " ['embrassezmoi', '#end'],\n",
              " ['moi', 'aussi', '#end'],\n",
              " ['ouvremoi', '#end'],\n",
              " ['ouvre', '#end'],\n",
              " ['parfait', '#end'],\n",
              " ['à', 'plus', '#end'],\n",
              " ['montremoi', '#end'],\n",
              " ['montrezmoi', '#end'],\n",
              " ['taisezvous', '#end'],\n",
              " ['fermela', '#end'],\n",
              " ['taistoi', '#end'],\n",
              " ['fermela', '#end'],\n",
              " ['la', 'ferme', '#end'],\n",
              " ['à', 'plus', 'tard', '#end'],\n",
              " ['prendsle', '#end'],\n",
              " ['prenezle', '#end'],\n",
              " ['dismoi', '#end'],\n",
              " ['ditesmoi', '#end'],\n",
              " ['tom', 'a', 'gagné', '#end'],\n",
              " ['réveilletoi', '#end'],\n",
              " ['réveilletoi', '#end'],\n",
              " ['réveillezvous', '#end'],\n",
              " ['réveilletoi', '#end'],\n",
              " ['réveillezvous', '#end'],\n",
              " ['lavetoi', '#end'],\n",
              " ['lavezvous', '#end'],\n",
              " ['nous', 'savons', '#end'],\n",
              " ['nous', 'perdîmes', '#end'],\n",
              " ['nous', 'avons', 'perdu', '#end'],\n",
              " ['nous', 'fûmes', 'battus', '#end'],\n",
              " ['nous', 'fûmes', 'battues', '#end'],\n",
              " ['nous', 'fûmes', 'défaits', '#end'],\n",
              " ['nous', 'fûmes', 'défaites', '#end'],\n",
              " ['nous', 'avons', 'été', 'défaits', '#end'],\n",
              " ['nous', 'avons', 'été', 'défaites', '#end'],\n",
              " ['nous', 'avons', 'été', 'battus', '#end'],\n",
              " ['nous', 'avons', 'été', 'battues', '#end'],\n",
              " ['bienvenue', '#end'],\n",
              " ['qui', 'a', 'gagné', '#end'],\n",
              " ['qui', 'la', 'emporté', '#end'],\n",
              " ['tu', 'cours', '#end'],\n",
              " ['suisje', 'gros', '#end'],\n",
              " ['suisje', 'grosse', '#end'],\n",
              " ['demandeleur', '#end'],\n",
              " ['demandezleur', '#end'],\n",
              " ['recule', '#end'],\n",
              " ['reculez', '#end'],\n",
              " ['recule', '#end'],\n",
              " ['reculez', '#end'],\n",
              " ['retiretoi', '#end'],\n",
              " ['retirezvous', '#end'],\n",
              " ['sois', 'un', 'homme', '#end'],\n",
              " ['soyez', 'un', 'homme', '#end'],\n",
              " ['sois', 'calme', '#end'],\n",
              " ['soyez', 'calme', '#end'],\n",
              " ['soyez', 'calmes', '#end'],\n",
              " ['aucune', 'idée', '#end'],\n",
              " ['jen', 'sais', 'foutre', 'rien', '#end'],\n",
              " ['appelle', 'tom', '#end'],\n",
              " ['appelez', 'tom', '#end'],\n",
              " ['courage', '#end'],\n",
              " ['détendstoi', '#end'],\n",
              " ['menottezle', '#end'],\n",
              " ['avance', '#end'],\n",
              " ['avancez', '#end'],\n",
              " ['continue', 'à', 'rouler', '#end'],\n",
              " ['continuez', 'à', 'rouler', '#end'],\n",
              " ['trouve', 'tom', '#end'],\n",
              " ['trouvez', 'tom', '#end'],\n",
              " ['réparez', 'ceci', '#end'],\n",
              " ['répare', 'ça', '#end'],\n",
              " ['lâchetoi', '#end'],\n",
              " ['descends', '#end'],\n",
              " ['descendez', '#end'],\n",
              " ['lâchetoi', '#end'],\n",
              " ['lâchezvous', '#end'],\n",
              " ['va', 'voir', 'ailleurs', 'si', 'jy', 'suis', '#end'],\n",
              " ['dégage', '#end'],\n",
              " ['va', 'au', 'diable', '#end'],\n",
              " ['sois', 'réaliste', '#end'],\n",
              " ['vasy', '#end'],\n",
              " ['poursuis', '#end'],\n",
              " ['passe', 'devant', '#end'],\n",
              " ['vasy', '#end'],\n",
              " ['bien', 'joué', '#end'],\n",
              " ['bon', 'boulot', '#end'],\n",
              " ['beau', 'travail', '#end'],\n",
              " ['attrapele', '#end'],\n",
              " ['attrapezle', '#end'],\n",
              " ['amusetoi', 'bien', '#end'],\n",
              " ['amusezvous', 'bien', '#end'],\n",
              " ['il', 'essaye', '#end'],\n",
              " ['il', 'est', 'mouillé', '#end'],\n",
              " ['aide', 'tom', '#end'],\n",
              " ['aidez', 'tom', '#end'],\n",
              " ['salut', 'les', 'mecs', '#end'],\n",
              " ['comme', 'cest', 'mignon', '#end'],\n",
              " ['quelle', 'profondeur', '#end'],\n",
              " ['comme', 'cest', 'chouette', '#end'],\n",
              " ['comme', 'cest', 'gentil', '#end'],\n",
              " ['cest', 'du', 'joli', '#end'],\n",
              " ['comme', 'cest', 'agréable', '#end'],\n",
              " ['faismoi', 'rire', '#end'],\n",
              " ['dépêchetoi', '#end'],\n",
              " ['grouille', '#end'],\n",
              " ['pressezvous', '#end'],\n",
              " ['fiça', '#end'],\n",
              " ['je', 'suis', 'gras', '#end'],\n",
              " ['je', 'men', 'suis', 'bien', 'sorti', '#end'],\n",
              " ['je', 'men', 'suis', 'bien', 'sortie', '#end'],\n",
              " ['je', 'lai', 'fait', '#end'],\n",
              " ['cest', 'moi', 'qui', 'lai', 'fait', '#end'],\n",
              " ['jai', 'échoué', '#end'],\n",
              " ['jai', 'oublié', '#end'],\n",
              " ['jai', 'compris', '#end'],\n",
              " ['jai', 'compris', '#end'],\n",
              " ['jai', 'capté', '#end'],\n",
              " ['jai', 'aidé', '#end'],\n",
              " ['jai', 'sauté', '#end'],\n",
              " ['regardé', '#end'],\n",
              " ['râlé', '#end'],\n",
              " ['fait', 'signe', 'de', 'la', 'tête', '#end'],\n",
              " ['obéi', '#end'],\n",
              " ['je', 'téléphonai', '#end'],\n",
              " ['jai', 'téléphoné', '#end'],\n",
              " ['je', 'refuse', '#end'],\n",
              " ['je', 'le', 'refuse', '#end'],\n",
              " ['je', 'me', 'suis', 'reposé', '#end'],\n",
              " ['je', 'me', 'suis', 'reposée', '#end'],\n",
              " ['je', 'lai', 'vu', '#end'],\n",
              " ['je', 'vu', '#end'],\n",
              " ['soupiré', '#end'],\n",
              " ['je', 'suis', 'resté', '#end'],\n",
              " ['je', 'suis', 'restée', '#end'],\n",
              " ['parlé', '#end'],\n",
              " ['je', 'lutilise', '#end'],\n",
              " ['jen', 'fais', 'usage', '#end'],\n",
              " ['je', 'men', 'sers', '#end'],\n",
              " ['je', 'paierai', '#end'],\n",
              " ['je', 'vais', 'essayer', '#end'],\n",
              " ['jessaierai', '#end'],\n",
              " ['je', 'suis', 'revenu', '#end'],\n",
              " ['me', 'revoilà', '#end'],\n",
              " ['je', 'suis', 'chauve', '#end'],\n",
              " ['je', 'suis', 'occupé', '#end'],\n",
              " ['je', 'suis', 'occupée', '#end'],\n",
              " ['je', 'suis', 'calme', '#end'],\n",
              " ['jai', 'froid', '#end'],\n",
              " ['je', 'suis', 'détendu', '#end'],\n",
              " ['je', 'suis', 'détendue', '#end'],\n",
              " ['je', 'suis', 'sourd', '#end'],\n",
              " ['je', 'suis', 'sourde', '#end'],\n",
              " ['jen', 'ai', 'fini', '#end'],\n",
              " ['je', 'suis', 'juste', '#end'],\n",
              " ['jai', 'la', 'peau', 'claire', '#end'],\n",
              " ['jai', 'le', 'teint', 'clair', '#end'],\n",
              " ['je', 'suis', 'rapide', '#end'],\n",
              " ['tout', 'va', 'bien', '#end'],\n",
              " ['je', 'vais', 'bien', '#end'],\n",
              " ['ça', 'va', '#end'],\n",
              " ['je', 'suis', 'libre', '#end'],\n",
              " ['je', 'suis', 'libre', '#end'],\n",
              " ['je', 'suis', 'disponible', '#end'],\n",
              " ['je', 'suis', 'repu', '#end'],\n",
              " ['je', 'suis', 'rassasié', '#end'],\n",
              " ['jen', 'suis', '#end'],\n",
              " ['je', 'suis', 'de', 'la', 'partie', '#end'],\n",
              " ['je', 'suis', 'content', '#end'],\n",
              " ['je', 'suis', 'chez', 'moi', '#end'],\n",
              " ['je', 'suis', 'en', 'retard', '#end'],\n",
              " ['je', 'suis', 'paresseux', '#end'],\n",
              " ['je', 'suis', 'fainéant', '#end'],\n",
              " ['je', 'suis', 'paresseuse', '#end'],\n",
              " ['je', 'suis', 'fainéante', '#end'],\n",
              " ['je', 'vais', 'bien', '#end'],\n",
              " ['je', 'me', 'porte', 'bien', '#end'],\n",
              " ['je', 'suis', 'riche', '#end'],\n",
              " ['je', 'suis', 'en', 'sécurité', '#end'],\n",
              " ['je', 'suis', 'malade', '#end'],\n",
              " ['jen', 'suis', 'certain', '#end'],\n",
              " ['je', 'suis', 'certain', '#end'],\n",
              " ['jen', 'suis', 'sûr', '#end'],\n",
              " ['jen', 'suis', 'sûre', '#end'],\n",
              " ['je', 'suis', 'grande', '#end'],\n",
              " ['je', 'suis', 'mince', '#end'],\n",
              " ['je', 'suis', 'ordonné', '#end'],\n",
              " ['je', 'suis', 'ordonnée', '#end'],\n",
              " ['je', 'suis', 'laid', '#end'],\n",
              " ['je', 'suis', 'laide', '#end'],\n",
              " ['je', 'suis', 'faible', '#end'],\n",
              " ['je', 'vais', 'bien', '#end'],\n",
              " ['je', 'me', 'porte', 'bien', '#end'],\n",
              " ['jai', 'gagné', '#end'],\n",
              " ['je', 'lai', 'emporté', '#end'],\n",
              " ['ça', 'aide', '#end'],\n",
              " ['ça', 'fait', 'mal', '#end'],\n",
              " ['elle', 'marche', '#end'],\n",
              " ['ça', 'fonctionne', '#end'],\n",
              " ['cest', 'tom', '#end'],\n",
              " ['cest', 'marrant', '#end'],\n",
              " ['cest', 'rigolo', '#end'],\n",
              " ['cest', 'le', 'sien', '#end'],\n",
              " ['cest', 'la', 'sienne', '#end'],\n",
              " ['cest', 'nouveau', '#end'],\n",
              " ['cest', 'neuf', '#end'],\n",
              " ['cest', 'bizarre', '#end'],\n",
              " ['il', 'est', 'rouge', '#end'],\n",
              " ['triste', '#end'],\n",
              " ['défense', 'dentrer', '#end'],\n",
              " ['nentrez', 'pas', '#end'],\n",
              " ['embrasse', 'tom', '#end'],\n",
              " ['laisse', 'tomber', '#end'],\n",
              " ['laissez', 'tomber', '#end'],\n",
              " ['laissezmoi', '#end'],\n",
              " ['laissenous', '#end'],\n",
              " ['laisseznous', '#end'],\n",
              " ['allonsy', '#end'],\n",
              " ['allonsy', '#end'],\n",
              " ['attention', '#end'],\n",
              " ['regarde', 'donc', '#end'],\n",
              " ['épousemoi', '#end'],\n",
              " ['épousezmoi', '#end'],\n",
              " ['puisje', 'partir', '#end'],\n",
              " ['puisje', 'y', 'aller', '#end'],\n",
              " ['puisje', 'my', 'rendre', '#end'],\n",
              " ['sauve', 'tom', '#end'],\n",
              " ['sauvez', 'tom', '#end'],\n",
              " ['elle', 'est', 'venue', '#end'],\n",
              " ['elle', 'est', 'morte', '#end'],\n",
              " ['elle', 'court', '#end'],\n",
              " ['assiedstoi', '#end'],\n",
              " ['asseyezvous', '#end'],\n",
              " ['assiedstoi', '#end'],\n",
              " ['assiedstoi', 'ici', '#end'],\n",
              " ['asseyezvous', 'ici', '#end'],\n",
              " ['parle', 'plus', 'fort', '#end'],\n",
              " ['parlez', 'plus', 'fort', '#end'],\n",
              " ['arrête', 'tom', '#end'],\n",
              " ['stoppez', 'tom', '#end'],\n",
              " ['goûtele', '#end'],\n",
              " ['goûtela', '#end'],\n",
              " ['goûtezle', '#end'],\n",
              " ['goûtezla', '#end'],\n",
              " ['disle', 'à', 'tom', '#end'],\n",
              " ['informezen', 'tom', '#end'],\n",
              " ['génial', '#end'],\n",
              " ['excellent', '#end'],\n",
              " ['formidable', '#end'],\n",
              " ['ils', 'gagnèrent', '#end'],\n",
              " ['elles', 'gagnèrent', '#end'],\n",
              " ['ils', 'ont', 'gagné', '#end'],\n",
              " ['elles', 'ont', 'gagné', '#end'],\n",
              " ['tom', 'est', 'venu', '#end'],\n",
              " ['tom', 'est', 'mort', '#end'],\n",
              " ['tom', 'muera', '#end'],\n",
              " ['tom', 'savait', '#end'],\n",
              " ['tom', 'est', 'parti', '#end'],\n",
              " ['tom', 'partit', '#end'],\n",
              " ['tom', 'a', 'menti', '#end'],\n",
              " ['tom', 'ment', '#end'],\n",
              " ['tom', 'a', 'perdu', '#end'],\n",
              " ['tom', 'a', 'payé', '#end'],\n",
              " ['trop', 'tard', '#end'],\n",
              " ['faitesmoi', 'confiance', '#end'],\n",
              " ['faismoi', 'confiance', '#end'],\n",
              " ['fais', 'un', 'effort', '#end'],\n",
              " ['essaiesen', '#end'],\n",
              " ['essayezen', '#end'],\n",
              " ['essaie', 'ceci', '#end'],\n",
              " ['essayez', 'ceci', '#end'],\n",
              " ['utilise', 'ceci', '#end'],\n",
              " ['utilisez', 'ceci', '#end'],\n",
              " ['emploie', 'ceci', '#end'],\n",
              " ['employez', 'ceci', '#end'],\n",
              " ['avertis', 'tom', '#end'],\n",
              " ['préviens', 'tom', '#end'],\n",
              " ['regardemoi', '#end'],\n",
              " ['regardezmoi', '#end'],\n",
              " ['regardeznous', '#end'],\n",
              " ['regardenous', '#end'],\n",
              " ['nous', 'sommes', 'daccord', '#end'],\n",
              " ['nous', 'irons', '#end'],\n",
              " ['pour', 'quoi', 'faire', '#end'],\n",
              " ['à', 'quoi', 'bon', '#end'],\n",
              " ['questce', 'quon', 'sest', 'marrés', '#end'],\n",
              " ['questce', 'quon', 'sest', 'marrées', '#end'],\n",
              " ['qui', 'est', 'venu', '#end'],\n",
              " ['qui', 'est', 'mort', '#end'],\n",
              " ['qui', 'est', 'tombé', '#end'],\n",
              " ['qui', 'démissionne', '#end'],\n",
              " ['qui', 'estil', '#end'],\n",
              " ['écrismoi', '#end'],\n",
              " ['écrivezmoi', '#end'],\n",
              " ['tu', 'as', 'perdu', '#end'],\n",
              " ['vous', 'avez', 'perdu', '#end'],\n",
              " ['après', 'vous', '#end'],\n",
              " ['en', 'joue', 'feu', '#end'],\n",
              " ['suisje', 'en', 'retard', '#end'],\n",
              " ['répondezmoi', '#end'],\n",
              " ['assiedstoi', '#end'],\n",
              " ['asseyezvous', '#end'],\n",
              " ['les', 'oiseaux', 'volent', '#end'],\n",
              " ['à', 'tes', 'souhaits', '#end'],\n",
              " ['appelle', 'à', 'la', 'maison', '#end'],\n",
              " ['calmezvous', '#end'],\n",
              " ['calmetoi', '#end'],\n",
              " ['pouvonsnous', 'partir', '#end'],\n",
              " ['pouvonsnous', 'nous', 'en', 'aller', '#end'],\n",
              " ['pouvonsnous', 'y', 'aller', '#end'],\n",
              " ['attrape', 'tom', '#end'],\n",
              " ['attrapez', 'tom', '#end'],\n",
              " ['rattrapele', '#end'],\n",
              " ['reviens', '#end'],\n",
              " ['revenez', '#end'],\n",
              " ['viens', 'ici', '#end'],\n",
              " ['venez', 'là', '#end'],\n",
              " ['viens', '#end'],\n",
              " ['venez', '#end'],\n",
              " ['venez', 'ici', '#end'],\n",
              " ['viens', 'chez', 'nous', '#end'],\n",
              " ['venez', 'chez', 'nous', '#end'],\n",
              " ['viens', 'chez', 'moi', '#end'],\n",
              " ['venez', 'chez', 'moi', '#end'],\n",
              " ['viens', 'bientôt', '#end'],\n",
              " ['venez', 'bientôt', '#end'],\n",
              " ['calmezvous', '#end'],\n",
              " ['aije', 'gagné', '#end'],\n",
              " ['laije', 'emporté', '#end'],\n",
              " ['estce', 'moi', 'qui', 'ai', 'gagné', '#end'],\n",
              " ['faitesle', 'maintenant', '#end'],\n",
              " ['des', 'chiens', 'aboient', '#end'],\n",
              " ['les', 'chiens', 'aboient', '#end'],\n",
              " ['ne', 'demande', 'pas', '#end'],\n",
              " ['ne', 'pleure', 'pas', '#end'],\n",
              " ['ne', 'meurs', 'pas', '#end'],\n",
              " ['ne', 'mourez', 'pas', '#end'],\n",
              " ['ne', 'mens', 'pas', '#end'],\n",
              " ['ne', 'courez', 'pas', '#end'],\n",
              " ['ne', 'cours', 'pas', '#end'],\n",
              " ['excusemoi', '#end'],\n",
              " ['excusezmoi', '#end'],\n",
              " ['fantastique', '#end'],\n",
              " ['sens', 'ça', '#end'],\n",
              " ['sentez', 'ça', '#end'],\n",
              " ['touche', 'ça', '#end'],\n",
              " ['touchez', 'ça', '#end'],\n",
              " ['suismoi', '#end'],\n",
              " ['suisnous', '#end'],\n",
              " ['suiveznous', '#end'],\n",
              " ['oublie', '#end'],\n",
              " ['oubliele', '#end'],\n",
              " ['oubliez', '#end'],\n",
              " ['oubliezle', '#end'],\n",
              " ['laisse', 'tomber', '#end'],\n",
              " ['oublie', '#end'],\n",
              " ['oubliele', '#end'],\n",
              " ['trouve', 'un', 'emploi', '#end'],\n",
              " ['trouve', 'un', 'boulot', '#end'],\n",
              " ['trouvez', 'un', 'emploi', '#end'],\n",
              " ['trouvez', 'un', 'boulot', '#end'],\n",
              " ['préparetoi', '#end'],\n",
              " ['préparezvous', '#end'],\n",
              " ['va', 'le', 'chercher', '#end'],\n",
              " ['allez', 'le', 'chercher', '#end'],\n",
              " ['entrez', '#end'],\n",
              " ['va', 'au', 'lit', '#end'],\n",
              " ['allez', 'au', 'lit', '#end'],\n",
              " ['bonne', 'chance', '#end'],\n",
              " ['bonne', 'chance', '#end'],\n",
              " ['attrape', 'ça', '#end'],\n",
              " ['attrapez', 'ça', '#end'],\n",
              " ['saisistoi', 'de', 'ça', '#end'],\n",
              " ['saisissezvous', 'de', 'ça', '#end'],\n",
              " ['attrape', 'ça', '#end'],\n",
              " ['attrapez', 'ça', '#end'],\n",
              " ['pas', 'touche', '#end'],\n",
              " ['il', 'est', 'malade', '#end'],\n",
              " ['il', 'est', 'vieux', '#end'],\n",
              " ['il', 'est', 'dj', '#end'],\n",
              " ['il', 'est', 'bon', '#end'],\n",
              " ['il', 'est', 'paresseux', '#end'],\n",
              " ['il', 'est', 'riche', '#end'],\n",
              " ['il', 'est', 'sexy', '#end'],\n",
              " ['me', 'voici', '#end'],\n",
              " ['voilà', 'cinq', 'dollars', '#end'],\n",
              " ['halte', 'au', 'feu', '#end'],\n",
              " ['cessez', 'le', 'feu', '#end'],\n",
              " ['tiens', 'ça', '#end'],\n",
              " ['tenez', 'ça', '#end'],\n",
              " ['tenez', 'ceci', '#end'],\n",
              " ['tiens', 'ceci', '#end'],\n",
              " ['cest', 'affreux', '#end'],\n",
              " ['comme', 'cest', 'bizarre', '#end'],\n",
              " ['comment', 'tom', 'vatil', '#end'],\n",
              " ['comment', 'va', 'tom', '#end'],\n",
              " ['je', 'suis', 'occupé', '#end'],\n",
              " ['je', 'suis', 'calme', '#end'],\n",
              " ['jai', 'froid', '#end'],\n",
              " ['je', 'vais', 'bien', '#end'],\n",
              " ['je', 'suis', 'bon', '#end'],\n",
              " ['je', 'suis', 'ici', '#end'],\n",
              " ['je', 'suis', 'paresseux', '#end'],\n",
              " ['je', 'suis', 'fainéant', '#end'],\n",
              " ['je', 'suis', 'paresseuse', '#end'],\n",
              " ['je', 'suis', 'fainéante', '#end'],\n",
              " ['je', 'vais', 'bien', '#end'],\n",
              " ['je', 'suis', 'malade', '#end'],\n",
              " ['je', 'suis', 'sûr', '#end'],\n",
              " ['je', 'suis', 'certain', '#end'],\n",
              " ['je', 'suis', 'faible', '#end'],\n",
              " ['je', 'vous', 'en', 'prie', '#end'],\n",
              " ['je', 'vous', 'en', 'conjure', '#end'],\n",
              " ['je', 'vous', 'en', 'supplie', '#end'],\n",
              " ['je', 'te', 'prie', '#end'],\n",
              " ['je', 'sais', 'courir', '#end'],\n",
              " ['je', 'sais', 'skier', '#end'],\n",
              " ['jeus', 'un', 'mouvement', 'de', 'recul', '#end'],\n",
              " ['jai', 'eu', 'un', 'mouvement', 'de', 'recul', '#end'],\n",
              " ['je', 'suis', 'rentré', 'en', 'moimême', '#end'],\n",
              " ['expiré', '#end'],\n",
              " ['jai', 'abandonné', '#end'],\n",
              " ['je', 'donne', 'ma', 'langue', 'au', 'chat', '#end'],\n",
              " ['jabandonne', '#end'],\n",
              " ['je', 'me', 'suis', 'mis', 'à', 'avoir', 'chaud', '#end'],\n",
              " ['je', 'me', 'suis', 'mise', 'à', 'avoir', 'chaud', '#end'],\n",
              " ['je', 'me', 'suis', 'amusé', '#end'],\n",
              " ['je', 'me', 'suis', 'amusée', '#end'],\n",
              " ['je', 'me', 'suis', 'marré', '#end'],\n",
              " ['je', 'me', 'suis', 'marrée', '#end'],\n",
              " ['je', 'déteste', 'ça', '#end'],\n",
              " ['je', 'lai', '#end'],\n",
              " ['jai', 'frappé', 'tom', '#end'],\n",
              " ['jespère', 'bien', '#end'],\n",
              " ['je', 'me', 'suis', 'dépêché', '#end'],\n",
              " ['je', 'me', 'suis', 'dépêchée', '#end'],\n",
              " ['inspiré', '#end'],\n",
              " ['je', 'le', 'savais', '#end'],\n",
              " ['jaime', 'ça', '#end'],\n",
              " ['je', 'perdu', '#end'],\n",
              " ['jadore', 'ça', '#end'],\n",
              " ['jadore', 'ça', '#end'],\n",
              " ['je', 'suis', 'sérieux', '#end'],\n",
              " ['je', 'suis', 'sérieux', '#end'],\n",
              " ['je', 'dois', 'y', 'aller', '#end'],\n",
              " ['il', 'faut', 'que', 'jy', 'aille', '#end'],\n",
              " ['il', 'me', 'faut', 'y', 'aller', '#end'],\n",
              " ['il', 'me', 'faut', 'partir', '#end'],\n",
              " ['il', 'me', 'faut', 'men', 'aller', '#end'],\n",
              " ['je', 'dois', 'partir', '#end'],\n",
              " ['je', 'dois', 'men', 'aller', '#end'],\n",
              " ['il', 'faut', 'que', 'je', 'men', 'aille', '#end'],\n",
              " ['jen', 'ai', 'besoin', '#end'],\n",
              " ['il', 'me', 'le', 'faut', '#end'],\n",
              " ['jai', 'remarqué', '#end'],\n",
              " ['jai', 'payé', 'davance', '#end'],\n",
              " ['je', 'le', 'promets', '#end'],\n",
              " ['je', 'me', 'suis', 'détendu', '#end'],\n",
              " ['je', 'me', 'suis', 'détendue', '#end'],\n",
              " ['jai', 'pris', 'ma', 'retraite', '#end'],\n",
              " ['jai', 'dit', 'non', '#end'],\n",
              " ['je', 'lai', 'dit', '#end'],\n",
              " ['je', 'lai', 'vu', '#end'],\n",
              " ['je', 'vu', '#end'],\n",
              " ['je', 'le', 'vis', '#end'],\n",
              " ['jen', 'ai', 'vu', 'une', '#end'],\n",
              " ['jen', 'ai', 'vu', 'un', '#end'],\n",
              " ['je', 'vous', 'vis', '#end'],\n",
              " ['je', 'te', 'vis', '#end'],\n",
              " ['je', 'tai', 'vue', '#end'],\n",
              " ['je', 'tai', 'vu', '#end'],\n",
              " ['je', 'vous', 'ai', 'vues', '#end'],\n",
              " ['je', 'vous', 'ai', 'vus', '#end'],\n",
              " ['je', 'vous', 'ai', 'vue', '#end'],\n",
              " ['je', 'vous', 'ai', 'vu', '#end'],\n",
              " ['je', 'vois', 'tom', '#end'],\n",
              " ['jai', 'crié', '#end'],\n",
              " ['jai', 'trébuché', '#end'],\n",
              " ['jai', 'plané', '#end'],\n",
              " ['je', 'le', 'veux', '#end'],\n",
              " ['jétais', 'nouveau', '#end'],\n",
              " ['jétais', 'nouvelle', '#end'],\n",
              " ['jirai', '#end'],\n",
              " ['je', 'me', 'suis', 'réveillé', '#end'],\n",
              " ['je', 'me', 'suis', 'éveillé', '#end'],\n",
              " ['je', 'serais', 'daccord', '#end'],\n",
              " ['je', 'partirais', '#end'],\n",
              " ['jappellerai', '#end'],\n",
              " ['je', 'cuisinerai', '#end'],\n",
              " ['jaiderai', '#end'],\n",
              " ['je', 'vivrai', '#end'],\n",
              " ['jobéirai', '#end'],\n",
              " ['je', 'ferai', 'mon', 'sac', '#end'],\n",
              " ['je', 'ferai', 'ma', 'valise', '#end'],\n",
              " ['je', 'plierai', 'mes', 'gaules', '#end'],\n",
              " ['je', 'passerai', '#end'],\n",
              " ['jabandonnerai', '#end'],\n",
              " ['je', 'chanterai', '#end'],\n",
              " ['jarrêterai', '#end'],\n",
              " ['je', 'nagerai', '#end'],\n",
              " ['je', 'parlerai', '#end'],\n",
              " ['je', 'vais', 'parler', '#end'],\n",
              " ['jattendrai', '#end'],\n",
              " ['je', 'marcherai', '#end'],\n",
              " ['je', 'vais', 'travailler', '#end'],\n",
              " ['je', 'travaillerai', '#end'],\n",
              " ['je', 'suis', 'flic', '#end'],\n",
              " ['je', 'suis', 'un', 'homme', '#end'],\n",
              " ['je', 'suis', 'en', 'vie', '#end'],\n",
              " ['je', 'suis', 'vivant', '#end'],\n",
              " ['je', 'suis', 'vivante', '#end'],\n",
              " ['je', 'suis', 'seule', '#end'],\n",
              " ['je', 'suis', 'seul', '#end'],\n",
              " ['je', 'suis', 'énervé', '#end'],\n",
              " ['je', 'suis', 'en', 'colère', '#end'],\n",
              " ['je', 'suis', 'armé', '#end'],\n",
              " ['je', 'suis', 'armée', '#end'],\n",
              " ['je', 'suis', 'réveillé', '#end'],\n",
              " ['je', 'suis', 'aveugle', '#end'],\n",
              " ['je', 'suis', 'fauché', '#end'],\n",
              " ['jai', 'décroché', '#end'],\n",
              " ['je', 'suis', 'propre', '#end'],\n",
              " ['je', 'suis', 'fou', '#end'],\n",
              " ['je', 'suis', 'folle', '#end'],\n",
              " ['je', 'suis', 'guéri', '#end'],\n",
              " ['je', 'suis', 'guérie', '#end'],\n",
              " ['jai', 'la', 'tête', 'qui', 'tourne', '#end'],\n",
              " ['je', 'suis', 'saoul', '#end'],\n",
              " ['je', 'suis', 'soûl', '#end'],\n",
              " ['je', 'suis', 'ivre', '#end'],\n",
              " ['je', 'me', 'meurs', '#end'],\n",
              " ['je', 'suis', 'en', 'avance', '#end'],\n",
              " ['je', 'suis', 'en', 'premier', '#end'],\n",
              " ['je', 'suis', 'difficile', '#end'],\n",
              " ['je', 'suis', 'tatillon', '#end'],\n",
              " ['je', 'suis', 'tatillonne', '#end'],\n",
              " ['je', 'pars', 'maintenant', '#end'],\n",
              " ['je', 'me', 'tire', '#end'],\n",
              " ['vais', '#end'],\n",
              " ['je', 'pars', '#end'],\n",
              " ['je', 'suis', 'loyal', '#end'],\n",
              " ['je', 'suis', 'loyale', '#end'],\n",
              " ['je', 'suis', 'veinard', '#end'],\n",
              " ['je', 'suis', 'veinarde', '#end'],\n",
              " ['jai', 'du', 'pot', '#end'],\n",
              " ['je', 'suis', 'chanceux', '#end'],\n",
              " ['je', 'suis', 'chanceuse', '#end'],\n",
              " ['je', 'suis', 'en', 'train', 'de', 'mentir', '#end'],\n",
              " ['je', 'suis', 'nu', '#end'],\n",
              " ['je', 'suis', 'nue', '#end'],\n",
              " ['je', 'me', 'trouve', 'nu', '#end'],\n",
              " ['je', 'me', 'trouve', 'nue', '#end'],\n",
              " ['je', 'suis', 'à', 'poil', '#end'],\n",
              " ['je', 'suis', 'tranquille', '#end'],\n",
              " ['je', 'suis', 'prête', '#end'],\n",
              " ['je', 'suis', 'prêt', '#end'],\n",
              " ['je', 'suis', 'prêt', '#end'],\n",
              " ['jai', 'raison', '#end'],\n",
              " ['je', 'suis', 'sobre', '#end'],\n",
              " ['excusemoi', '#end'],\n",
              " ['désolé', '#end'],\n",
              " ['excusezmoi', '#end'],\n",
              " ['désolé', '#end'],\n",
              " ['je', 'suis', 'désolé', '#end'],\n",
              " ['je', 'suis', 'désolée', '#end'],\n",
              " ['je', 'suis', 'coincée', '#end'],\n",
              " ['je', 'suis', 'timide', '#end'],\n",
              " ['je', 'suis', 'fatigué', '#end'],\n",
              " ['je', 'suis', 'dur', '#end'],\n",
              " ['je', 'suis', 'dure', '#end'],\n",
              " ['je', 'suis', 'dur', 'à', 'cuire', '#end'],\n",
              " ['je', 'suis', 'dure', 'à', 'cuire', '#end'],\n",
              " ['je', 'suis', 'à', 'toi', '#end'],\n",
              " ['je', 'suis', 'à', 'vous', '#end'],\n",
              " ['jai', 'perdu', '#end'],\n",
              " ['estce', 'que', 'tom', 'va', 'bien', '#end'],\n",
              " ['tom', 'vatil', 'bien', '#end'],\n",
              " ['cest', 'grave', '#end'],\n",
              " ['estce', 'éloigné', '#end'],\n",
              " ['estce', 'loin', '#end'],\n",
              " ['estce', 'chaud', '#end'],\n",
              " ['estce', 'toi', '#end'],\n",
              " ['estce', 'vous', '#end'],\n",
              " ['estce', 'que', 'cest', 'vous', '#end'],\n",
              " ['ça', 'a', 'échoué', '#end'],\n",
              " ['cest', 'nouveau', '#end'],\n",
              " ['cest', 'neuf', '#end'],\n",
              " ['il', 'a', 'neigé', '#end'],\n",
              " ['ça', 'sent', 'mauvais', '#end'],\n",
              " ['ça', 'pue', '#end'],\n",
              " ['cétait', 'bon', '#end'],\n",
              " ['cétait', 'correct', '#end'],\n",
              " ['cétait', 'ok', '#end'],\n",
              " ['ça', 'a', 'fonctionné', '#end'],\n",
              " ['ça', 'a', 'marché', '#end'],\n",
              " ['il', 'est', 'trois', 'heures', 'et', 'demie', '#end'],\n",
              " ['il', 'est', 'huit', 'heures', 'trente', '#end'],\n",
              " ['il', 'est', 'h', '#end'],\n",
              " ['cest', 'une', 'télé', '#end'],\n",
              " ['il', 'fait', 'froid', '#end'],\n",
              " ['cest', 'froid', '#end'],\n",
              " ['cest', 'sombre', '#end'],\n",
              " ['elle', 'est', 'morte', '#end'],\n",
              " ['cest', 'mort', '#end'],\n",
              " ['il', 'est', 'mort', '#end'],\n",
              " ['cest', 'fait', '#end'],\n",
              " ['cest', 'simple', '#end'],\n",
              " ['cest', 'de', 'la', 'nourriture', '#end'],\n",
              " ['cest', 'gratuit', '#end'],\n",
              " ['elle', 'est', 'ici', '#end'],\n",
              " ['cest', 'ici', '#end'],\n",
              " ['cest', 'le', 'sien', '#end'],\n",
              " ['cest', 'la', 'sienne', '#end'],\n",
              " ['il', 'est', 'tard', '#end'],\n",
              " ['cest', 'perdu', '#end'],\n",
              " ['cest', 'le', 'mien', '#end'],\n",
              " ['cest', 'la', 'mienne', '#end'],\n",
              " ['cest', 'à', 'moi', '#end'],\n",
              " ['il', 'sagit', 'du', 'mien', '#end'],\n",
              " ['cest', 'ouvert', '#end'],\n",
              " ['cest', 'le', 'nôtre', '#end'],\n",
              " ['cest', 'la', 'nôtre', '#end'],\n",
              " ['cest', 'à', 'nous', '#end'],\n",
              " ['cest', 'du', 'sable', '#end'],\n",
              " ['il', 'est', 'temps', '#end'],\n",
              " ['cest', 'lheure', '#end'],\n",
              " ['cest', 'vrai', '#end'],\n",
              " ['cest', 'du', 'boulot', '#end'],\n",
              " ['garde', 'ça', '#end'],\n",
              " ['gardez', 'cela', '#end'],\n",
              " ['garde', 'ça', '#end'],\n",
              " ['gardez', 'ceci', '#end'],\n",
              " ['ainsi', 'soitil', '#end'],\n",
              " ['laisse', 'faire', '#end'],\n",
              " ['laissemoi', 'partir', '#end'],\n",
              " ['laissezmoi', 'partir', '#end'],\n",
              " ['lâchemoi', '#end'],\n",
              " ['lâchezmoi', '#end'],\n",
              " ['laissemoi', 'men', 'aller', '#end'],\n",
              " ['laissezmoi', 'men', 'aller', '#end'],\n",
              " ['laissezmoi', 'y', 'aller', '#end'],\n",
              " ['laissemoi', 'y', 'aller', '#end'],\n",
              " ['laissemoi', 'partir', '#end'],\n",
              " ['laissezmoi', 'partir', '#end'],\n",
              " ['laissemoi', 'men', 'aller', '#end'],\n",
              " ['laissezmoi', 'men', 'aller', '#end'],\n",
              " ['laissezmoi', 'rentrer', '#end'],\n",
              " ['laissezmoi', 'entrer', '#end'],\n",
              " ['demandons', '#end'],\n",
              " ['mangeons', '#end'],\n",
              " ['voyons', 'voir', '#end'],\n",
              " ['reste', 'allongé', 'immobile', '#end'],\n",
              " ['reste', 'allongée', 'immobile', '#end'],\n",
              " ['restez', 'allongé', 'immobile', '#end'],\n",
              " ['restez', 'allongée', 'immobile', '#end'],\n",
              " ['restez', 'allongés', 'immobiles', '#end'],\n",
              " ['restez', 'allongées', 'immobiles', '#end'],\n",
              " ['regarde', 'derrière', 'toi', '#end'],\n",
              " ['regarde', 'derrière', '#end'],\n",
              " ['regarde', 'ici', '#end'],\n",
              " ['regardez', 'ici', '#end'],\n",
              " ['échauffetoi', '#end'],\n",
              " ['échauffezvous', '#end'],\n",
              " ['détendstoi', '#end'],\n",
              " ['laissetoi', 'aller', '#end'],\n",
              " ['laissezvous', 'aller', '#end'],\n",
              " ['bouge', 'de', 'là', '#end'],\n",
              " ['poussezvous', '#end'],\n",
              " ['poussetoi', '#end'],\n",
              " ['joli', 'coup', '#end'],\n",
              " ['pour', 'sûr', '#end'],\n",
              " ['mais', 'ouais', '#end'],\n",
              " ['pardi', '#end'],\n",
              " ['bien', 'sûr', '#end'],\n",
              " ['pour', 'sûr', '#end'],\n",
              " ['je', 'vous', 'en', 'prie', '#end'],\n",
              " ['je', 'ten', 'prie', '#end'],\n",
              " ['pardon', '#end'],\n",
              " ['je', 'vous', 'demande', 'pardon', '#end'],\n",
              " ['plaîtil', '#end'],\n",
              " ['lis', 'ceci', '#end'],\n",
              " ['dis', 'bonjour', '#end'],\n",
              " ['voyez', 'cidessus', '#end'],\n",
              " ['voyez', 'cidessous', '#end'],\n",
              " ['voir', 'cidessous', '#end'],\n",
              " ['capturezle', '#end'],\n",
              " ['attrapezle', '#end'],\n",
              " ['vraiment', '#end'],\n",
              " ['estce', 'sérieux', '#end'],\n",
              " ['sérieusement', '#end'],\n",
              " ['elle', 'pleurait', '#end'],\n",
              " ['elle', 'pleura', '#end'],\n",
              " ['elle', 'a', 'essayé', '#end'],\n",
              " ['elle', 'marche', '#end'],\n",
              " ['elle', 'est', 'chaude', '#end'],\n",
              " ['elle', 'est', 'très', 'attirante', '#end'],\n",
              " ['signe', 'ici', '#end'],\n",
              " ['signez', 'ici', '#end'],\n",
              " ['signe', 'ça', '#end'],\n",
              " ['signez', 'ceci', '#end'],\n",
              " ['ralentis', '#end'],\n",
              " ['ralentissez', '#end'],\n",
              " ['reste', 'en', 'arrière', '#end'],\n",
              " ['restez', 'en', 'arrière', '#end'],\n",
              " ['restez', 'calme', '#end'],\n",
              " ['reste', 'calme', '#end'],\n",
              " ['garde', 'ton', 'calme', '#end'],\n",
              " ['garde', 'ton', 'sangfroid', '#end'],\n",
              " ['reste', 'tranquille', '#end'],\n",
              " ['reste', 'baissé', '#end'],\n",
              " ['restez', 'baissé', '#end'],\n",
              " ['reste', 'baissé', '#end'],\n",
              " ['restez', 'baissé', '#end'],\n",
              " ['restez', 'là', '#end'],\n",
              " ['reste', 'ici', '#end'],\n",
              " ['restez', 'ici', '#end'],\n",
              " ['reste', 'mince', '#end'],\n",
              " ['recule', '#end'],\n",
              " ['reculez', '#end'],\n",
              " ['arrêtez', '#end'],\n",
              " ['arrêtez', 'ça', '#end'],\n",
              " ['arrête', 'ça', '#end'],\n",
              " ['arrêtezles', '#end'],\n",
              " ['prends', 'soin', 'de', 'toi', '#end'],\n",
              " ['soyez', 'prudente', '#end'],\n",
              " ['prends', 'soin', 'de', 'toi', '#end'],\n",
              " ['prenez', 'soin', 'de', 'vous', '#end'],\n",
              " ['prends', 'le', 'mien', '#end'],\n",
              " ['prends', 'la', 'mienne', '#end'],\n",
              " ['prenez', 'le', 'mien', '#end'],\n",
              " ['prenez', 'la', 'mienne', '#end'],\n",
              " ['prends', 'les', 'miens', '#end'],\n",
              " ['prends', 'les', 'miennes', '#end'],\n",
              " ['prenez', 'les', 'miens', '#end'],\n",
              " ['prenez', 'les', 'miennes', '#end'],\n",
              " ['prends', 'ça', '#end'],\n",
              " ['prenez', 'ça', '#end'],\n",
              " ['merci', '#end'],\n",
              " ['il', 'ny', 'a', 'pas', 'de', 'problème', '#end'],\n",
              " ['pas', 'de', 'problème', '#end'],\n",
              " ['ce', 'nest', 'pas', 'grave', '#end'],\n",
              " ['ça', 'va', '#end'],\n",
              " ['cest', 'ça', '#end'],\n",
              " ['ils', 'sont', 'tombés', '#end'],\n",
              " ['elles', 'sont', 'tombées', '#end'],\n",
              " ['ils', 'sont', 'partis', '#end'],\n",
              " ['elles', 'sont', 'parties', '#end'],\n",
              " ['ils', 'ont', 'menti', '#end'],\n",
              " ['elles', 'ont', 'menti', '#end'],\n",
              " ['ils', 'ont', 'perdu', '#end'],\n",
              " ['elles', 'ont', 'perdu', '#end'],\n",
              " ['ils', 'nageaient', '#end'],\n",
              " ['elles', 'nageaient', '#end'],\n",
              " ['ils', 'nagèrent', '#end'],\n",
              " ['elles', 'nagèrent', '#end'],\n",
              " ['le', 'temps', 'est', 'écoulé', '#end'],\n",
              " ['tom', 'cuisine', '#end'],\n",
              " ['tom', 'pleura', '#end'],\n",
              " ['tom', 'va', 'bien', '#end'],\n",
              " ['tom', 'tricote', '#end'],\n",
              " ['tom', 'sait', '#end'],\n",
              " ['tom', 'assure', '#end'],\n",
              " ['tom', 'a', 'parlé', '#end'],\n",
              " ['tom', 'marche', '#end'],\n",
              " ['tom', 'a', 'fait', 'signe', 'de', 'la', 'main', '#end'],\n",
              " ['tom', 'est', 'en', 'train', 'de', 'travailler', '#end'],\n",
              " ['tom', 'est', 'gros', '#end'],\n",
              " ['tom', 'est', 'en', 'colère', '#end'],\n",
              " ['tom', 'est', 'fou', '#end'],\n",
              " ['tom', 'est', 'triste', '#end'],\n",
              " ['fais', 'confiance', 'à', 'tom', '#end'],\n",
              " ['faites', 'confiance', 'à', 'tom', '#end'],\n",
              " ['essaie', 'encore', '#end'],\n",
              " ['essayez', 'de', 'nouveau', '#end'],\n",
              " ['essaie', 'de', 'nouveau', '#end'],\n",
              " ['essaiele', '#end'],\n",
              " ['tourne', 'à', 'gauche', '#end'],\n",
              " ['attends', 'ici', '#end'],\n",
              " ['attends', 'là', '#end'],\n",
              " ['attendez', 'ici', '#end'],\n",
              " ['attendez', 'là', '#end'],\n",
              " ['attention', '#end'],\n",
              " ['faites', 'attention', '#end'],\n",
              " ['fais', 'attention', '#end'],\n",
              " ['nous', 'sommes', 'tombés', 'daccord', '#end'],\n",
              " ['nous', 'avons', 'réussi', '#end'],\n",
              " ['nous', 'avons', 'réussi', '#end'],\n",
              " ['nous', 'lavons', 'fait', '#end'],\n",
              " ['nous', 'avons', 'oublié', '#end'],\n",
              " ['nous', 'lavons', 'vu', '#end'],\n",
              " ['nous', 'lavons', 'vue', '#end'],\n",
              " ['nous', 'avons', 'souri', '#end'],\n",
              " ['nous', 'discutâmes', '#end'],\n",
              " ['nous', 'avons', 'discuté', '#end'],\n",
              " ['nous', 'nous', 'sommes', 'entretenus', '#end'],\n",
              " ['nous', 'nous', 'sommes', 'entretenues', '#end'],\n",
              " ['nous', 'nous', 'entretînmes', '#end'],\n",
              " ['nous', 'attendîmes', '#end'],\n",
              " ['nous', 'avons', 'attendu', '#end'],\n",
              " ['nous', 'essayerons', '#end'],\n",
              " ['nous', 'tenterons', '#end'],\n",
              " ['nous', 'lemporterons', '#end'],\n",
              " ['nous', 'gagnerons', '#end'],\n",
              " ['nous', 'avons', 'chaud', '#end'],\n",
              " ['nous', 'sommes', 'tristes', '#end'],\n",
              " ['nous', 'sommes', 'timides', '#end'],\n",
              " ['bien', 'vu', '#end'],\n",
              " ['bien', 'cuit', '#end'],\n",
              " ['à', 'la', 'bonne', 'heure', '#end'],\n",
              " ['bien', 'joué', '#end'],\n",
              " ['pas', 'mal', '#end'],\n",
              " ['quoi', '#end'],\n",
              " ['ça', 'va', '#end'],\n",
              " ['quoi', 'de', 'beau', '#end'],\n",
              " ['qui', 'sen', 'préoccupe', '#end'],\n",
              " ['qui', 'sen', 'soucie', '#end'],\n",
              " ['à', 'qui', 'ceci', 'importetil', '#end'],\n",
              " ['qui', 'estce', '#end'],\n",
              " ['qui', 'estil', '#end'],\n",
              " ['qui', 'estce', '#end'],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob6oWOBPYzif",
        "colab_type": "text"
      },
      "source": [
        "> Cree un conjunto de validación y de pruebas fijos de $N_{exp} = 10000$ datos ¿Cuántos datos quedan para entrenar? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJKDAcVjYzig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBd4f1uKYzik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_exp= 10000\n",
        "\n",
        "\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(texts_input, texts_output,\n",
        "                                                            test_size=N_exp, random_state=22)\n",
        "X_train_l, X_val_l, Y_train_l, Y_val_l = train_test_split(X_train_l, Y_train_l, \n",
        "                                                          test_size=N_exp, random_state=22)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE4URGT5Yzio",
        "colab_type": "code",
        "outputId": "2b1911d4-24e8-4223-936c-22e229db29e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test_l[1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'gave', 'me', 'a', 'ride', 'home']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH7C0x6FYzir",
        "colab_type": "code",
        "outputId": "565f18aa-60b5-4b93-b303-1011680185bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_test_l[1]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['il', 'ma', 'raccompagnée', 'chez', 'moi', 'en', 'voiture', '#end']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPR55ezIYziu",
        "colab_type": "code",
        "outputId": "e3b31e9a-77cb-4d4b-8e4f-a30328396f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_test_l)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGUEmrunYzix",
        "colab_type": "code",
        "outputId": "922a9e3d-c3d5-4ae8-e203-846facd0944d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_val_l)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV7qILZVYzi0",
        "colab_type": "code",
        "outputId": "795079ff-2c94-4ecd-c4e3-9f13a319f450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_train_l)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "147130"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT1DYZbIYzi9",
        "colab_type": "text"
      },
      "source": [
        "> c) Genere un vocabulario, **desde el conjunto de entrenamiento**, sobre las palabras a recibir y generar en la traducción, esto es codificarlas a un valor entero que servirá para que la red las vea en una representación útil a procesar, *comience desde el 1 debido a que el cero será utilizado más adelante*. Para reducir el vocabulario considere las palabras que aparecen un mínimo de *min_count* veces en todo los datos, se aconseja un valor de 3. Comente sobre la importancia de ésto al reducir el vocabulario ¿De qué tamaño es el vocabulario de entrada y salida? ¿La diferencia de ésto podría ser un factor importante?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MrFbMKBYzi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vocab(texts, min_count=1):\n",
        "    count_vocab = {}\n",
        "    for sentence in texts:\n",
        "        for word in sentence:\n",
        "            if word not in count_vocab:\n",
        "                count_vocab[word] = 1\n",
        "            else:\n",
        "                count_vocab[word] += 1\n",
        "    return [word for word,count in count_vocab.items() if count >= min_count]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXcxJwZpkrgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_source = create_vocab(X_train_l, min_count=3)\n",
        "word2idx_s = {w: i+1 for i, w in enumerate(vocab_source)} #index (i+1) start from 1,2,3,...\n",
        "idx2word_s = {i+1: w for i, w in enumerate(vocab_source)}\n",
        "n_words_s = len(vocab_source)\n",
        "vocab_target = create_vocab(Y_train_l, min_count=3)\n",
        "word2idx_t = {w: i+1 for i, w in enumerate(vocab_target)}  #Converting text to numbers\n",
        "idx2word_t = {i+1: w for i, w in enumerate(vocab_target)}  #Converting number to text\n",
        "n_words_t = len(vocab_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRRF7v05ktYU",
        "colab_type": "code",
        "outputId": "e8af04ec-938e-438f-ddaf-266e277229b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('El vocabulario  de entrenamiento de Ingles (source) tiene un largo' ,  n_words_s, 'palabras')\n",
        "print('El vocabulario  de entrenamiento de Frances (target) tiene un largo' ,  n_words_t, 'palabras')\n",
        "print('Hay una diferencia de', n_words_s-n_words_t, 'entre los dos conjuntos')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El vocabulario  de entrenamiento de Ingles (source) tiene un largo 7339 palabras\n",
            "El vocabulario  de entrenamiento de Frances (target) tiene un largo 12338 palabras\n",
            "Hay una diferencia de -4999 entre los dos conjuntos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi5yl4ztYzjH",
        "colab_type": "text"
      },
      "source": [
        "Existe una diferencia clara entre el vocabulario de entrada y de salida, es lógico que esto pase ya que ambas lenguas tienen orgines lexicos distintos. Esto no necesariamente va a perjudicar al estudio, ya que no todas las palabras tienen como traducción solo una palabra y hay casos en que dos palabras pueden ser traducciones válidas dependiendo del contexto.  \n",
        "Como anotación para el 'Source' el vocabulario se reduce en 5000 palabras aproximadamente, pero en el 'Target' se ignora la mitad.\n",
        "\n",
        "Ignorar ciertas palabras poco comúnes, como en español xilofono, ayuda a evitar el overfitting o que se aprendan características de poco valor para el modelo, pero si se ignoran muchas palabras también caemos en ofecfittig.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQurecvkYzjI",
        "colab_type": "text"
      },
      "source": [
        ">Ahora codifique las palabras a los números indexados con el vocabulario. Recuerde que si una palabra en los otros conjuntos, o en el mismo de entrenamiento, no aparece en el vocabulario no se podrá generar una codificación, por lo que será **ignorada** ¿Cómo se podría evitar ésto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ3ZUrz1YzjM",
        "colab_type": "text"
      },
      "source": [
        "En estos casos se podría manejar un diccionario de traducción inmediata, que entregue un resultado predeterminado dependiendo de la palabra del source o target que fue ignorada en los conjuntos de entrenamiento. Esto reduciría la precisión pero al menos permitiría que toda la frase fuera reescrita en el idioma de llegada, con un mayor o menor grado de precisión.\n",
        "\n",
        "Otra alternativa es hacer una doble limpieza de las frases en el source y en target que contengan estas palabras poco comunes, pero esto puede reducir en sobremanera el dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ-KFH6SYzjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"\"\" Source/input data \"\"\"\n",
        "dataX_train = [[word2idx_s[word] for word in sent if word in word2idx_s] for sent in X_train_l]\n",
        "dataX_valid = [[word2idx_s[word] for word in sent if word in word2idx_s] for sent in X_val_l]\n",
        "dataX_test = [[word2idx_s[word] for word in sent if word in word2idx_s] for sent in X_test_l]\n",
        "\n",
        "\n",
        "#\"\"\" Target/output data \"\"\"\n",
        "dataY_train = [[word2idx_t[word] for word in sent if word in word2idx_t] for sent in Y_train_l]\n",
        "dataY_valid = [[word2idx_t[word] for word in sent if word in word2idx_t] for sent in Y_val_l] \n",
        "dataY_test = [[word2idx_t[word] for word in sent if word in word2idx_t] for sent in Y_test_l] \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx2ffpT7YzjQ",
        "colab_type": "code",
        "outputId": "3f8b1ada-c9ae-4e9c-ac3f-e4ed8ebe2804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "dataX_train[0:8]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 5, 6],\n",
              " [7, 8, 5, 9],\n",
              " [10, 8, 11, 12, 13],\n",
              " [1, 14, 15, 1, 16, 3],\n",
              " [17, 18, 19],\n",
              " [10, 20, 21, 22],\n",
              " [1, 23, 24, 25, 26, 27, 28, 29, 30],\n",
              " [1, 31, 32, 27, 33, 34]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ_KJbVSYzjS",
        "colab_type": "text"
      },
      "source": [
        "> d) Debido al largo variable de los textos de entrada y salida será necesario estandarizar ésto para poder trabajar de manera más cómoda en Keras, *cada texto (entrada y salida) pueden tener distinto largo máximo*. Comente sobre la decisión del tipo de *padding*, *pre o post* ¿Qué sucede al variar el largo máximo de instantes de tiempo para procesar en cada parte del modelo (entrada y salida)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMvvfkbWYzjT",
        "colab_type": "text"
      },
      "source": [
        "Hasta no probar las dos alternativas, a priorí no sabemos como afectará al aprendizaje del modelo. Sin embargo, buscando sobre el tema, encontramos que se recomienda que el input tenga pre padding y el output tenga postpadding, ya que supuestamente que las \"palabras\" estén juntas es mejor para el desempeño del modelo.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "¿Qué sucede al variar el largo máximo de instantes de tiempo para procesar en cada parte del modelo (entrada y salida)? : \n",
        ">| El modelo encoder decoder se va a encargar de que se adapten los largos acorde al idioma en el que se esta trabajando para que todo tome sentido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs69ryyuYzjW",
        "colab_type": "code",
        "outputId": "bcd65360-c22b-4817-af3e-5bb1676e190d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "print('Dato preprocesados')\n",
        "\n",
        "\"\"\" INPUT DATA (Origin language) \"\"\"\n",
        "max_inp_length = max(map(len,dataX_train))\n",
        "print(\"Largo max Source(Ingles): \",max_inp_length)\n",
        "word2idx_s[\"*\"] = 0 #padding symbol\n",
        "idx2word_s[0] = \"*\"\n",
        "n_words_s += 1  \n",
        "\n",
        "\n",
        "X_train = sequence.pad_sequences(dataX_train, maxlen=max_inp_length, padding='pre', value=word2idx_s[\"*\"])\n",
        "X_valid = sequence.pad_sequences(dataX_valid, maxlen=max_inp_length, padding='pre', value=word2idx_s[\"*\"])\n",
        "X_test = sequence.pad_sequences(dataX_test, maxlen=max_inp_length, padding='pre', value=word2idx_s[\"*\"])\n",
        "\n",
        "\n",
        "\"\"\" OUTPUT DATA (Destination language) \"\"\"\n",
        "max_out_length = max(map(len,dataY_train)) \n",
        "print(\"Largo max Target(Fances): \",max_out_length)\n",
        "word2idx_t[\"*\"] = 0 #padding symbol\n",
        "idx2word_t[0] = \"*\"\n",
        "n_words_t += 1  \n",
        "\n",
        "Y_train = sequence.pad_sequences(dataY_train, maxlen=max_out_length, padding='post', value=word2idx_t[\"*\"])\n",
        "Y_valid = sequence.pad_sequences(dataY_valid, maxlen=max_out_length, padding='post', value=word2idx_t[\"*\"])\n",
        "Y_test = sequence.pad_sequences(dataY_test, maxlen=max_out_length, padding='post', value=word2idx_t[\"*\"])\n",
        "\n",
        "\n",
        "print('El vocabulario  de entrenamiento de Ingles (source) tiene un largo' ,  n_words_s, 'palabras')\n",
        "print('El vocabulario  de entrenamiento de Frances (target) tiene un largo' ,  n_words_t, 'palabras')\n",
        "print('Hay una diferencia de', n_words_s-n_words_t, 'entre los dos conjuntos')\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dato preprocesados\n",
            "Largo max Source(Ingles):  44\n",
            "Largo max Target(Fances):  54\n",
            "El vocabulario  de entrenamiento de Ingles (source) tiene un largo 7340 palabras\n",
            "El vocabulario  de entrenamiento de Frances (target) tiene un largo 12339 palabras\n",
            "Hay una diferencia de -4999 entre los dos conjuntos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6yKJxJoYzjZ",
        "colab_type": "text"
      },
      "source": [
        "max inp length y max out length se usaran despues para definir un largo estandar de las frases, en ambos idiomas, para el encoder decorder, con esto se pueden conectar capas densas y planas (\"flatten\"). \n",
        "Se aumenta la dimensionalidad de los Y, para que sea compatible con la dimensión de la capa TimeDistributed que se usará en el modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHR-oQQYzja",
        "colab_type": "code",
        "outputId": "c40ced31-7ac9-46cf-c62e-600f45f4c77f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "X_test[0:5]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,   31,   91,   11,  617,   27,  534],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,   10, 1320,  204,   11, 1356,   81],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,   55,  131,  291,   74,  537,  129],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,  125,   23,   27,   73,   47,  639],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,   10,  117,  588,  293,   27,  429,   53]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxcR543BpTla",
        "colab_type": "code",
        "outputId": "80fe815d-d784-4f97-9863-cdb70051670a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "Y_test[0:5]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  41,  590,  394,  102,   81,   10,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [  29,  232,   96,  264,    8,  141,   10,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [  71, 2259,    3,  328,   62, 1504,   57,   97,   10,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [  24,  995,   85,   87,  769,   10,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [  29,  482,   43,   57,  239,  643,  499,   10,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKBCY3kTYzjc",
        "colab_type": "text"
      },
      "source": [
        "> e) Para evitar que la red obtenga una ganancia por imitar/predecir el símbolo de *padding* que está bastante presente en los datos coloque un peso sobre éste clase, con valor 0, así se evita que tenga impacto en la función objetivo. Ya que *keras* no soporta directamente ésto en series de tiempo coloque el peso a cada instante de tiempo de cada dato de entrenamiento dependiendo de su clase. Comente sobre alguna otra forma en que se podría manejar el evitar que la red prediga en mayoría el símbolo de *padding*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Poi5O80mYzje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_weights = np.ones(n_words_t)\n",
        "c_weights[0] = 0 #padding class masked\n",
        "sample_weight = np.zeros(Y_train.shape)\n",
        "for i in range(sample_weight.shape[0]):\n",
        "    sample_weight[i] = c_weights[Y_train[i,:]]\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQWZEhkbYzji",
        "colab_type": "text"
      },
      "source": [
        "Primero c_weights genera un arreglo de ceros del largo de las palabras en el target, se le da peso 0 a la primera palabra, que justamente es el padding que utilizamos y reservamos anteriormente.\n",
        "sample_weight, genera una matriz de pesoso para cada palabra en el entrenamiento, se estandarizan los pesos de todas las palabras como 0.\n",
        "Es una matriz de pesoso para el conjunto Y de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q08cenlzYzjq",
        "colab_type": "text"
      },
      "source": [
        "> f) Para lograr la tarea defina una red recurrente del tipo *encoder*-*decoder* como la que se presenta en la siguiente imágen.\n",
        "<img src=\"https://chunml.github.io/ChunML.github.io/images/projects/sequence-to-sequence/repeated_vector.png\" width=\"60%\" />\n",
        "En primer lugar defina el *Encoder* que procesara el texto de entrada y retornará un solo vector final, haciendo uso de las capas ya conocidas de *Embedding* para generar un vector denso de palabra y *GRU*, pero en su versión acelerada para GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHfAG90NYzj0",
        "colab_type": "text"
      },
      "source": [
        "Luego defina la sección que conecta el largo (*timesteps*) de entrada *vs* el de salida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P63TOhMgYzj3",
        "colab_type": "text"
      },
      "source": [
        "Finalmente defina el *Decoder* para generar la secuencia de salida en texto de palabras en otro idioma, a través de la función *softmax* sobre cada instante de tiempo (*timestep*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxcLeavoYzj6",
        "colab_type": "text"
      },
      "source": [
        "Entrene la red entre 1 a 5 *epochs*, agregando los pesos definidos sobre cada ejemplo de entrenamiento. Además de utilizar una función de pérdida que evita generar explícitamente los *one hot vector*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptX2E9EhYzjr",
        "colab_type": "code",
        "outputId": "11e4ba34-29a3-443e-eedd-ab3ac1f111d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_inp_length"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rINZgsyBYzjw",
        "colab_type": "code",
        "outputId": "b84c6d0b-435f-4070-cdb6-d0c47afe775e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_words_s"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7340"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwGIslp3Yzjz",
        "colab_type": "text"
      },
      "source": [
        "Se crea el Encoder que recive como entrada el vector del diccionario de ingles, y sale un vector de dimención 100 denso que cambia la representación de las oraciones.\n",
        "Las capas de CuDNNGRU, son GRU rapidas (gate recurrent unit)\n",
        "La última capa retorna solo el el último output de la secuencia.\n",
        "Este encoder sirve para darle un contexto al vocabulario del diccionario en ingles, que palabras tienden a ir juntar y en que orden, etc.\n",
        "\n",
        "El Vector repetidor, simplemente repite el vector que recive para que tenga la misma dimensión que las frases en frances (max_out_length)\n",
        "\n",
        "El decoder funciona de manera similar, pero ambas capas CuDNNGRU entregan la lista de outputs, y es la capa final, timedistributed, la que hace una capa densa con el largo del diccionario de frances (target)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpmDCpkysaXP",
        "colab_type": "text"
      },
      "source": [
        "Se usa la función de perdida sparse_categorical_crossentropy, ya que con eso no tenemos que generar 'one-hot vector', y así evaluamos sobre los valores enteros de la salida.\n",
        "Los 5 modelos probados tienen un desempeño similar considerando el val_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUxPuhv5Yzj4",
        "colab_type": "code",
        "outputId": "54964249-835b-4f41-d2fc-73af89ecec47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "#from keras.layers import CuDNNGRU, TimeDistributed,Dense\n",
        "EMBEDDING_DIM = 100\n",
        "learning_rate = 1e-3\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_length))\n",
        "model.add(CuDNNGRU(64, return_sequences=True))\n",
        "model.add(CuDNNGRU(128, return_sequences=False))\n",
        "\n",
        "\n",
        "model.add(RepeatVector(max_out_length)) #conection\n",
        "\n",
        "model.add(CuDNNGRU(128, return_sequences=True))\n",
        "model.add(CuDNNGRU(64, return_sequences=True))\n",
        "\n",
        "model.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0805 17:43:05.535273 140170182502272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0805 17:43:05.576995 140170182502272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0805 17:43:05.585782 140170182502272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 44, 100)           734000    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (None, 44, 64)            31872     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_2 (CuDNNGRU)       (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 54, 128)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_3 (CuDNNGRU)       (None, 54, 128)           99072     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_4 (CuDNNGRU)       (None, 54, 64)            37248     \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 54, 12339)         802035    \n",
            "=================================================================\n",
            "Total params: 1,778,723\n",
            "Trainable params: 1,778,723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlbpqY_yYzj7",
        "colab_type": "code",
        "outputId": "bda865e9-4ca0-4b51-9297-66c345114423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147130, 54)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xiVfiJsep-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test=Y_test[:,:,None]\n",
        "Y_valid=Y_valid[:,:,None]\n",
        "Y_train=Y_train[:,:,None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofdYePr5gKhS",
        "colab_type": "code",
        "outputId": "48c48fe2-e51d-48d8-c55e-6299c44f9361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147130, 54, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u01BVNK1Yzj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.contrib.rnn import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yqqFomUYzkA",
        "colab_type": "code",
        "outputId": "fc471336-65ae-45d2-d498-12d8c339c427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mejor_epoch = 0\n",
        "mejor_desempeño=1000\n",
        "for i in range(5):\n",
        "  \n",
        "  EMBEDDING_DIM = 100\n",
        "  learning_rate = 1e-3\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_length))\n",
        "  model.add(CuDNNGRU(64, return_sequences=True))\n",
        "  model.add(CuDNNGRU(128, return_sequences=False))\n",
        "\n",
        "\n",
        "  model.add(RepeatVector(max_out_length)) #conection\n",
        "\n",
        "  model.add(CuDNNGRU(128, return_sequences=True))\n",
        "  model.add(CuDNNGRU(64, return_sequences=True))\n",
        "\n",
        "  model.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n",
        "  model.summary()\n",
        "  \n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')#, metrics = ['accuracy'])\n",
        "  h = model.fit(X_train, Y_train, epochs=i+1, batch_size=256, verbose= 1, validation_data=(X_valid, Y_valid), sample_weight = sample_weight) \n",
        "  desempeño= h.history[\"val_loss\"]\n",
        "  if desempeño[-1] <mejor_desempeño:\n",
        "    mejor_desempeño=desempeño[-1]\n",
        "    mejor_epoch=i+1\n",
        "  print('Para', i+1, 'epochs, el desempeño fue:', desempeño)\n",
        "  \n",
        "\n",
        "  \n",
        "#    evaluate(x=X_train, y=Y_train, batch_size=256, verbose=1, sample_weight=sample_weight)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0805 17:43:08.257223 140170182502272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0805 17:43:08.280075 140170182502272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0805 17:43:08.385264 140170182502272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 44, 100)           734000    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_5 (CuDNNGRU)       (None, 44, 64)            31872     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_6 (CuDNNGRU)       (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 54, 128)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_7 (CuDNNGRU)       (None, 54, 128)           99072     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_8 (CuDNNGRU)       (None, 54, 64)            37248     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 54, 12339)         802035    \n",
            "=================================================================\n",
            "Total params: 1,778,723\n",
            "Trainable params: 1,778,723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0805 17:43:08.856670 140170182502272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 147130 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "147130/147130 [==============================] - 123s 837us/step - loss: 6.2593 - val_loss: 13.6146\n",
            "Para 1 epochs, el desempeño fue: [13.61455868988037]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 44, 100)           734000    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_9 (CuDNNGRU)       (None, 44, 64)            31872     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_10 (CuDNNGRU)      (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 54, 128)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_11 (CuDNNGRU)      (None, 54, 128)           99072     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_12 (CuDNNGRU)      (None, 54, 64)            37248     \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 54, 12339)         802035    \n",
            "=================================================================\n",
            "Total params: 1,778,723\n",
            "Trainable params: 1,778,723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 147130 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "147130/147130 [==============================] - 118s 800us/step - loss: 6.2601 - val_loss: 13.5062\n",
            "Epoch 2/2\n",
            "147130/147130 [==============================] - 117s 792us/step - loss: 6.0829 - val_loss: 14.4446\n",
            "Para 2 epochs, el desempeño fue: [13.506239094543457, 14.444597378540038]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 44, 100)           734000    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_13 (CuDNNGRU)      (None, 44, 64)            31872     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_14 (CuDNNGRU)      (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "repeat_vector_4 (RepeatVecto (None, 54, 128)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_15 (CuDNNGRU)      (None, 54, 128)           99072     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_16 (CuDNNGRU)      (None, 54, 64)            37248     \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 54, 12339)         802035    \n",
            "=================================================================\n",
            "Total params: 1,778,723\n",
            "Trainable params: 1,778,723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 147130 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "147130/147130 [==============================] - 118s 801us/step - loss: 6.2595 - val_loss: 13.6060\n",
            "Epoch 2/3\n",
            "147130/147130 [==============================] - 117s 792us/step - loss: 6.0486 - val_loss: 14.5472\n",
            "Epoch 3/3\n",
            "147130/147130 [==============================] - 117s 793us/step - loss: 5.9340 - val_loss: 14.7189\n",
            "Para 3 epochs, el desempeño fue: [13.605958161926269, 14.547198913574219, 14.718874879455566]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 44, 100)           734000    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_17 (CuDNNGRU)      (None, 44, 64)            31872     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_18 (CuDNNGRU)      (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "repeat_vector_5 (RepeatVecto (None, 54, 128)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_19 (CuDNNGRU)      (None, 54, 128)           99072     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_20 (CuDNNGRU)      (None, 54, 64)            37248     \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 54, 12339)         802035    \n",
            "=================================================================\n",
            "Total params: 1,778,723\n",
            "Trainable params: 1,778,723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 147130 samples, validate on 10000 samples\n",
            "Epoch 1/4\n",
            "147130/147130 [==============================] - 118s 802us/step - loss: 6.2568 - val_loss: 13.6184\n",
            "Epoch 2/4\n",
            "147130/147130 [==============================] - 117s 793us/step - loss: 6.0819 - val_loss: 14.5576\n",
            "Epoch 3/4\n",
            "147130/147130 [==============================] - 117s 792us/step - loss: 6.0174 - val_loss: 14.7298\n",
            "Epoch 4/4\n",
            "147130/147130 [==============================] - 117s 792us/step - loss: 5.8684 - val_loss: 14.7153\n",
            "Para 4 epochs, el desempeño fue: [13.618411222839356, 14.557648640441894, 14.729776959228516, 14.715324923706055]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 44, 100)           734000    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_21 (CuDNNGRU)      (None, 44, 64)            31872     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_22 (CuDNNGRU)      (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "repeat_vector_6 (RepeatVecto (None, 54, 128)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_23 (CuDNNGRU)      (None, 54, 128)           99072     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_24 (CuDNNGRU)      (None, 54, 64)            37248     \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 54, 12339)         802035    \n",
            "=================================================================\n",
            "Total params: 1,778,723\n",
            "Trainable params: 1,778,723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 147130 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "147130/147130 [==============================] - 118s 804us/step - loss: 6.2595 - val_loss: 13.5403\n",
            "Epoch 2/5\n",
            "147130/147130 [==============================] - 117s 793us/step - loss: 6.0250 - val_loss: 14.4868\n",
            "Epoch 3/5\n",
            "147130/147130 [==============================] - 117s 793us/step - loss: 5.8936 - val_loss: 14.6455\n",
            "Epoch 4/5\n",
            "147130/147130 [==============================] - 117s 794us/step - loss: 5.6440 - val_loss: 14.6623\n",
            "Epoch 5/5\n",
            "147130/147130 [==============================] - 117s 795us/step - loss: 5.2985 - val_loss: 14.6059\n",
            "Para 5 epochs, el desempeño fue: [13.540321726989745, 14.486767645263672, 14.645516564941406, 14.66228623199463, 14.605899635314941]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-omjbPXiXPx7",
        "colab_type": "code",
        "outputId": "b7e02653-9287-41e6-8bd2-be792cb85bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "  EMBEDDING_DIM = 100\n",
        "  learning_rate = 1e-3\n",
        "  model1 = Sequential()\n",
        "  model1.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_length))\n",
        "  model1.add(CuDNNGRU(64, return_sequences=True))\n",
        "  model1.add(CuDNNGRU(128, return_sequences=False))\n",
        "\n",
        "\n",
        "  model1.add(RepeatVector(max_out_length)) #conection\n",
        "\n",
        "  model1.add(CuDNNGRU(128, return_sequences=True))\n",
        "  model1.add(CuDNNGRU(64, return_sequences=True))\n",
        "\n",
        "  model1.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n",
        "  model1.summary()\n",
        "  \n",
        "  model1.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')#, metrics = ['accuracy'])\n",
        "  h = model1.fit(X_train, Y_train, epochs=5, batch_size=256, verbose= 1, validation_data=(X_valid, Y_valid), sample_weight = sample_weight) \n",
        "  desempeño= h.history[\"val_loss\"]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 44, 100)           734000    \n",
            "_________________________________________________________________\n",
            "cu_dnngru_29 (CuDNNGRU)      (None, 44, 64)            31872     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_30 (CuDNNGRU)      (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "repeat_vector_8 (RepeatVecto (None, 54, 128)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_31 (CuDNNGRU)      (None, 54, 128)           99072     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_32 (CuDNNGRU)      (None, 54, 64)            37248     \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 54, 12339)         802035    \n",
            "=================================================================\n",
            "Total params: 1,778,723\n",
            "Trainable params: 1,778,723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 147130 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "147130/147130 [==============================] - 119s 809us/step - loss: 5.8977 - val_loss: 14.6305\n",
            "Epoch 2/5\n",
            "147130/147130 [==============================] - 117s 795us/step - loss: 5.1971 - val_loss: 13.8511\n",
            "Epoch 3/5\n",
            "147130/147130 [==============================] - 117s 794us/step - loss: 4.8728 - val_loss: 13.7546\n",
            "Epoch 4/5\n",
            "147130/147130 [==============================] - 117s 795us/step - loss: 4.6228 - val_loss: 14.3914\n",
            "Epoch 5/5\n",
            "147130/147130 [==============================] - 117s 796us/step - loss: 4.4104 - val_loss: 14.0124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA0E76FZzy-A",
        "colab_type": "code",
        "outputId": "ac6afb66-d68d-457f-ecb0-3b67ad38f051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "desempeño[-1]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.012416613769531"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HJunS2x1cwO",
        "colab_type": "text"
      },
      "source": [
        "el largo de los datos de entrada y salida, así como el tamañano del conjutno de entrenamiento afecta al tiempo.\n",
        "Se podrían partir la frases más largas, ya que varias frases estan formadas por oraciones más pequeñas que son autocontenidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Y_ZO-uYzkF",
        "colab_type": "text"
      },
      "source": [
        "> g) Debido a lo costoso de tener una red completamente recurrente para entrenar y poder experimentar, cambie el modelo que procesa el *Encoder* por una red convolucional, reduciendo el número de capas pero aumentando las neuronas. Utilice tamaños de *kernel*  igual a 5 y funciones de activaciones relu. Se agregan capas de *BatchNormalization* debido a que en el *Decoder* contamos con redes recurrentes que tienen capa activación distinta a la usada por las convoluciones. La capa de *GlobalMaxPooling1d* es lo que permite reducir toda la información extraída a un único vector, como se realizó anteriormente con *return_sequences=False*, comente sobre la ganancia o desventaja de ésto *vs* la red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkuyh6soYzkJ",
        "colab_type": "text"
      },
      "source": [
        "Entrene el modelo igual a lo presentado anteriormente pero ahora por 20 *epochs* ¿Cambian los tiempos de procesamiento y la cantidad de parámetros?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGDyA9SZYzkG",
        "colab_type": "code",
        "outputId": "a5b3d9e5-abfd-4067-bca1-00812bff6f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "from keras.layers import Conv1D,MaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D,BatchNormalization\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_length))\n",
        "model2.add(Conv1D(256, 5, padding='same', activation='relu', strides=1))\n",
        "model2.add(BatchNormalization()) #for stability\n",
        "model2.add(Conv1D(256, 5, padding='same', activation='relu', strides=1))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(GlobalMaxPooling1D()) #aka to return_sequences=False\n",
        "model2.add(RepeatVector(max_out_length)) #conection\n",
        "model2.add(CuDNNGRU(256, return_sequences=True))\n",
        "model2.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n",
        "model2.summary() "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 44, 100)           734000    \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 44, 256)           128256    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 44, 256)           1024      \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 44, 256)           327936    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 44, 256)           1024      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_10 (RepeatVect (None, 54, 256)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_34 (CuDNNGRU)      (None, 54, 256)           394752    \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 54, 12339)         3171123   \n",
            "=================================================================\n",
            "Total params: 4,758,115\n",
            "Trainable params: 4,757,091\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d6h6huEYzkK",
        "colab_type": "code",
        "outputId": "6a5ef24a-6ab0-448c-e232-cf10d8fc43c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "#optimizer = 'rmsprop'\n",
        "  \n",
        "model2.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')#, metrics = ['accuracy'])\n",
        "h2=model2.fit(X_train, Y_train, epochs=20, batch_size=256,validation_data=(X_valid, Y_valid), sample_weight = sample_weight)\n",
        "    "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 147130 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "147130/147130 [==============================] - 169s 1ms/step - loss: 4.9064 - val_loss: 14.4272\n",
            "Epoch 2/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 3.8372 - val_loss: 14.4161\n",
            "Epoch 3/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 3.3923 - val_loss: 14.3383\n",
            "Epoch 4/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 3.1177 - val_loss: 14.2519\n",
            "Epoch 5/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.9301 - val_loss: 14.3043\n",
            "Epoch 6/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.7882 - val_loss: 14.1891\n",
            "Epoch 7/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.6651 - val_loss: 14.2041\n",
            "Epoch 8/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.5681 - val_loss: 14.3009\n",
            "Epoch 9/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.4820 - val_loss: 14.2915\n",
            "Epoch 10/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.4096 - val_loss: 14.2712\n",
            "Epoch 11/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.3380 - val_loss: 14.2625\n",
            "Epoch 12/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.2780 - val_loss: 14.2863\n",
            "Epoch 13/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.2222 - val_loss: 14.2945\n",
            "Epoch 14/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.1653 - val_loss: 14.2922\n",
            "Epoch 15/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.1172 - val_loss: 14.2978\n",
            "Epoch 16/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.0758 - val_loss: 14.2868\n",
            "Epoch 17/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.0409 - val_loss: 14.2902\n",
            "Epoch 18/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 2.0072 - val_loss: 14.2971\n",
            "Epoch 19/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 1.9737 - val_loss: 14.2975\n",
            "Epoch 20/20\n",
            "147130/147130 [==============================] - 165s 1ms/step - loss: 1.9386 - val_loss: 14.2963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StuXzDsB2X-O",
        "colab_type": "text"
      },
      "source": [
        "Una diferencia obvia entre los dos modelos es la cantidad de parámetros que aparecen, el modelo1 tenía cerca de 1.700.000 parametros entrenables, sin haber ninguno no entrenable.\n",
        "Pero el modelo 2 tiene casi el triple, aproximadamente 4.750.000 parametros entrenables, y 1024 no entrenables, que vienen de la capa de batch normalization.\n",
        "\n",
        "Además el entrenamiento de los epoch dura más, y su desempeño no es mejor que le modelo original usado.\n",
        "No solo eso, el desempeño con  solo dos epochs entrenados, es cercano al mejor desempeño logrado. Lo que nos hace creer que se llega a un overfitting, donde la loss sigue mejorando, pero la val_loss no, aunque en nuestro caso se mantiene estable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl2TbhnyYzkO",
        "colab_type": "text"
      },
      "source": [
        "> h) Visualice lo aprendido por el modelo sobre algunos datos del conjunto de entrenamiento y validación, comente lo observado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFUq-jvcYzkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_words(y_indexs, data=\"target\"):\n",
        "    \"\"\" Predict until '-#end-' is seen \"\"\"\n",
        "    return_val = []\n",
        "    for indx_word in y_indexs:\n",
        "        if indx_word != 0: #start to predict\n",
        "            return_val.append(np.squeeze(indx_word))\n",
        "            if data == \"target\": #if target is predicting\n",
        "                if indx_word == word2idx_t[\"#end\"]:\n",
        "                    return return_val                \n",
        "    return return_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdAwe0AM3iEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c99202c5-23f3-4fee-f98c-d6d348d6ce49"
      },
      "source": [
        "n_s = 50\n",
        "\n",
        "X_set=X_train\n",
        "Y_set=Y_train\n",
        "\n",
        "idx = np.random.choice(np.arange(Y_set.shape[0]), size=n_s, replace=False)\n",
        "\n",
        "Y_set_pred = model.predict_classes(X_set[idx] )\n",
        "Y_set_pred2 = model2.predict_classes(X_set[idx] )\n",
        "\n",
        "for i, n_sampled in enumerate(idx):\n",
        "    text_input = [idx2word_s[p] for p in predict_words(X_set[n_sampled], data=\"source\")]\n",
        "    print(\"Texto source: \", ' '.join(text_input))\n",
        "    text_real = [idx2word_t[p] for p in predict_words(Y_set[n_sampled,:,0], data=\"target\")]\n",
        "    print(\"Texto target real: \", ' '.join( text_real))\n",
        "    text_sampled = [idx2word_t[p] for p in predict_words(Y_set_pred[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 1: \", ' '.join(text_sampled))\n",
        "    text_sampled2 = [idx2word_t[p] for p in predict_words(Y_set_pred2[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 2: \", ' '.join(text_sampled2))\n",
        "    \n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto source:  will you please help me\n",
            "Texto target real:  je vous prie #end\n",
            "Texto target predicho del modelo 1:  comment que que #end\n",
            "Texto target predicho del modelo 2:  pourraistu je vous prie prie prie prie prie prie prie prie #end\n",
            "Texto source:  you said you were going to handle it\n",
            "Texto target real:  vous avez dit que vous alliez vous en saisir #end\n",
            "Texto target predicho del modelo 1:  tom que que que que que #end\n",
            "Texto target predicho del modelo 2:  vous avez dit que vous vous allais en en #end\n",
            "Texto source:  i think its time for me to start cooking dinner\n",
            "Texto target real:  je pense quil est temps que je commence à préparer le dîner #end\n",
            "Texto target predicho del modelo 1:  je ne que que que de de de de de #end\n",
            "Texto target predicho del modelo 2:  je pense quil est temps que moi commence à à à le dîner dîner dîner dîner dîner dîner dîner dîner #end\n",
            "Texto source:  dont touch the stove\n",
            "Texto target real:  ne touche pas la cuisinière #end\n",
            "Texto target predicho del modelo 1:  je pas pas de #end\n",
            "Texto target predicho del modelo 2:  ne touche pas le touche #end\n",
            "Texto source:  tom doesnt want to go anywhere today\n",
            "Texto target real:  tom na envie daller nulle part aujourdhui #end\n",
            "Texto target predicho del modelo 1:  tom ne pas pas pas #end\n",
            "Texto target predicho del modelo 2:  tom ne envie envie nulle part #end\n",
            "Texto source:  ive never told anyone\n",
            "Texto target real:  je ne lai jamais dit à personne #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas que #end\n",
            "Texto target predicho del modelo 2:  je ne lai jamais dit à quiconque #end\n",
            "Texto source:  dont put your elbows on the table\n",
            "Texto target real:  ne mets pas les coudes sur la table #end\n",
            "Texto target predicho del modelo 1:  je pas pas de de la #end\n",
            "Texto target predicho del modelo 2:  ne mets pas les chaussures sur la sur #end\n",
            "Texto source:  mary is helping her mother\n",
            "Texto target real:  marie aide sa mère #end\n",
            "Texto target predicho del modelo 1:  cest est de de #end\n",
            "Texto target predicho del modelo 2:  mary sa sa mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère\n",
            "Texto source:  can you take a photo of us\n",
            "Texto target real:  peuxtu nous prendre en photo #end\n",
            "Texto target predicho del modelo 1:  tom que de de de #end\n",
            "Texto target predicho del modelo 2:  peuxtu nous prendre en photo #end\n",
            "Texto source:  be nicer to your sister\n",
            "Texto target real:  sois plus gentille avec ta sœur #end\n",
            "Texto target predicho del modelo 1:  comment de de de #end\n",
            "Texto target predicho del modelo 2:  soyez plus gentil avec votre sœur #end\n",
            "Texto source:  its sweet\n",
            "Texto target real:  cest gentil #end\n",
            "Texto target predicho del modelo 1:  il est #end\n",
            "Texto target predicho del modelo 2:  cest gentil #end\n",
            "Texto source:  give me a hand with this\n",
            "Texto target real:  donnemoi un coup de main pour ça #end\n",
            "Texto target predicho del modelo 1:  estce de #end\n",
            "Texto target predicho del modelo 2:  donnezmoi un coup à coup #end\n",
            "Texto source:  try not to inhale the fumes\n",
            "Texto target real:  essaie de ne pas respirer les vapeurs #end\n",
            "Texto target predicho del modelo 1:  ce ne pas pas pas de #end\n",
            "Texto target predicho del modelo 2:  essaie de ne pas les les #end\n",
            "Texto source:  youre selfish\n",
            "Texto target real:  vous êtes égoïstes #end\n",
            "Texto target predicho del modelo 1:  tu me #end\n",
            "Texto target predicho del modelo 2:  vous êtes égoïste #end\n",
            "Texto source:  they started working right away\n",
            "Texto target real:  ils se mirent immédiatement à travailler #end\n",
            "Texto target predicho del modelo 1:  il est de de #end\n",
            "Texto target predicho del modelo 2:  ils ont commencé commencé travailler travailler #end\n",
            "Texto source:  that bridge is made of stone\n",
            "Texto target real:  ce pont est de pierre #end\n",
            "Texto target predicho del modelo 1:  il est est de #end\n",
            "Texto target predicho del modelo 2:  ce pont est de pierre #end\n",
            "Texto source:  its an extreme case\n",
            "Texto target real:  cest un cas extrême #end\n",
            "Texto target predicho del modelo 1:  il est de #end\n",
            "Texto target predicho del modelo 2:  cest un affaire affaire #end\n",
            "Texto source:  were getting out of here the storm is coming\n",
            "Texto target real:  nous sortons dici la tempête approche #end\n",
            "Texto target predicho del modelo 1:  il a de de de de #end\n",
            "Texto target predicho del modelo 2:  nous allons dici la tempête #end\n",
            "Texto source:  tell me why you were absent from school yesterday\n",
            "Texto target real:  ditesmoi pourquoi vous étiez absente de lécole hier #end\n",
            "Texto target predicho del modelo 1:  comment que que que de de #end\n",
            "Texto target predicho del modelo 2:  ditesmoi pourquoi vous étiez absent hier lécole hier hier #end\n",
            "Texto source:  youre not going are you\n",
            "Texto target real:  vous ny allez pas si #end\n",
            "Texto target predicho del modelo 1:  tom ne pas pas #end\n",
            "Texto target predicho del modelo 2:  tu ne vas pas si #end\n",
            "Texto source:  i found the picture you were looking for\n",
            "Texto target real:  jai trouvé le tableau que tu cherchais #end\n",
            "Texto target predicho del modelo 1:  je a de de que #end\n",
            "Texto target predicho del modelo 2:  jai trouvé la photo que tu tu #end\n",
            "Texto source:  can i go home now\n",
            "Texto target real:  puisje rentrer à la maison maintenant #end\n",
            "Texto target predicho del modelo 1:  comment que de de #end\n",
            "Texto target predicho del modelo 2:  puisje aller chez la maison maintenant maintenant #end\n",
            "Texto source:  shes by far the tallest girl\n",
            "Texto target real:  elle est de loin la plus grande fille #end\n",
            "Texto target predicho del modelo 1:  il a de de la la #end\n",
            "Texto target predicho del modelo 2:  elle est de loin fille fille fille fille fille #end\n",
            "Texto source:  what time do you leave home in the morning\n",
            "Texto target real:  à quelle heure partezvous de chez vous le matin #end\n",
            "Texto target predicho del modelo 1:  cest que de de de de de de #end\n",
            "Texto target predicho del modelo 2:  à quelle heure chez chez chez chez à matin #end\n",
            "Texto source:  tom cut his finger on a piece of glass\n",
            "Texto target real:  tom sest coupé le doigt sur un morceau de verre #end\n",
            "Texto target predicho del modelo 1:  tom est de de de la la #end\n",
            "Texto target predicho del modelo 2:  tom sest coupé le doigt sur un un un #end\n",
            "Texto source:  i know your name\n",
            "Texto target real:  je connais votre nom #end\n",
            "Texto target predicho del modelo 1:  je suis de #end\n",
            "Texto target predicho del modelo 2:  je connais ton nom #end\n",
            "Texto source:  theres a restaurant pretty close to here but its not very good\n",
            "Texto target real:  il y a un restaurant assez près dici mais il nest pas très bon #end\n",
            "Texto target predicho del modelo 1:  il a de de de de de de que que que que pas #end\n",
            "Texto target predicho del modelo 2:  il y a un un assez mais mais mais mais assez assez assez assez assez #end\n",
            "Texto source:  count the apples in the basket\n",
            "Texto target real:  compte les pommes dans le panier #end\n",
            "Texto target predicho del modelo 1:  la la de de la #end\n",
            "Texto target predicho del modelo 2:  dans dans pommes dans les panier #end\n",
            "Texto source:  i really need a drink now\n",
            "Texto target real:  jai vraiment besoin dun verre maintenant #end\n",
            "Texto target predicho del modelo 1:  je suis de de de #end\n",
            "Texto target predicho del modelo 2:  jai vraiment besoin dun dun verre maintenant #end\n",
            "Texto source:  i liked tom\n",
            "Texto target real:  jai aimé tom #end\n",
            "Texto target predicho del modelo 1:  je me #end\n",
            "Texto target predicho del modelo 2:  jai aimé tom #end\n",
            "Texto source:  that toy is made of wood\n",
            "Texto target real:  ce jouet est en bois #end\n",
            "Texto target predicho del modelo 1:  cest est est de #end\n",
            "Texto target predicho del modelo 2:  ce fait est fait en en #end\n",
            "Texto source:  im just a plain old office worker\n",
            "Texto target real:  je ne suis quun simple employé de bureau #end\n",
            "Texto target predicho del modelo 1:  je suis pas de de la #end\n",
            "Texto target predicho del modelo 2:  je suis suis un bureau bureau bureau bureau bureau #end\n",
            "Texto source:  were not serious\n",
            "Texto target real:  nous ne sommes pas sérieuses #end\n",
            "Texto target predicho del modelo 1:  il ne pas pas #end\n",
            "Texto target predicho del modelo 2:  nous ne sommes pas sérieux #end\n",
            "Texto source:  dont make noise\n",
            "Texto target real:  ne fais pas de bruit #end\n",
            "Texto target predicho del modelo 1:  je ne pas #end\n",
            "Texto target predicho del modelo 2:  ne fais pas de bruit #end\n",
            "Texto source:  its a very stressful job\n",
            "Texto target real:  cest un boulot très stressant #end\n",
            "Texto target predicho del modelo 1:  il est de #end\n",
            "Texto target predicho del modelo 2:  cest un très très très #end\n",
            "Texto source:  its cheaper to take the bus\n",
            "Texto target real:  cest moins cher de prendre le bus #end\n",
            "Texto target predicho del modelo 1:  il est de de de #end\n",
            "Texto target predicho del modelo 2:  cest un de de prendre prendre le #end\n",
            "Texto source:  the price is right\n",
            "Texto target real:  le prix est exact #end\n",
            "Texto target predicho del modelo 1:  la est est #end\n",
            "Texto target predicho del modelo 2:  le prix est prix #end\n",
            "Texto source:  i will sleep at my aunts\n",
            "Texto target real:  je dormirai chez ma tante #end\n",
            "Texto target predicho del modelo 1:  je suis de de #end\n",
            "Texto target predicho del modelo 2:  je à à ma #end\n",
            "Texto source:  we did not from monkeys we share a common\n",
            "Texto target real:  nous navons pas évolué à partir des singes nous partageons un commun #end\n",
            "Texto target predicho del modelo 1:  il ne pas pas de de de de de #end\n",
            "Texto target predicho del modelo 2:  nous navons pas fait un un navons des nous un #end\n",
            "Texto source:  do you have health insurance\n",
            "Texto target real:  astu une assurance médicale #end\n",
            "Texto target predicho del modelo 1:  comment de de #end\n",
            "Texto target predicho del modelo 2:  êtesvous une assurance une assurance #end\n",
            "Texto source:  let me do my job\n",
            "Texto target real:  laissemoi faire mon boulot #end\n",
            "Texto target predicho del modelo 1:  comment de #end\n",
            "Texto target predicho del modelo 2:  laissezmoi faire mon travail #end\n",
            "Texto source:  when did you tell me that\n",
            "Texto target real:  quand me lastu dit #end\n",
            "Texto target predicho del modelo 1:  comment que que que #end\n",
            "Texto target predicho del modelo 2:  quand me dit dit #end\n",
            "Texto source:  i love to party\n",
            "Texto target real:  jadore faire la fête #end\n",
            "Texto target predicho del modelo 1:  tom de #end\n",
            "Texto target predicho del modelo 2:  jadore faire la #end\n",
            "Texto source:  she agreed with him on what to do with the old car\n",
            "Texto target real:  elle a été daccord avec lui sur ce quil convenait de faire avec la vieille voiture #end\n",
            "Texto target predicho del modelo 1:  il a de de de de de de de #end\n",
            "Texto target predicho del modelo 2:  elle était daccord sur lui avec ce quil quil faire vieille faire faire avec #end\n",
            "Texto source:  i dont know who they are but they dont look friendly\n",
            "Texto target real:  jignore qui ils sont mais ils ne semblent pas amicaux #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas pas que que que que que #end\n",
            "Texto target predicho del modelo 2:  je ne ils sont ils ils ils semblent ils ils semblent #end\n",
            "Texto source:  the floor was covered with dust\n",
            "Texto target real:  le sol était couvert de poussière #end\n",
            "Texto target predicho del modelo 1:  la est est de #end\n",
            "Texto target predicho del modelo 2:  le sol était couvert de poussière #end\n",
            "Texto source:  i saw you in the park yesterday\n",
            "Texto target real:  je vous ai vu dans le parc hier #end\n",
            "Texto target predicho del modelo 1:  je suis pas de de #end\n",
            "Texto target predicho del modelo 2:  je vous ai au le parc hier hier #end\n",
            "Texto source:  i received your note\n",
            "Texto target real:  jai reçu ta note #end\n",
            "Texto target predicho del modelo 1:  je le de #end\n",
            "Texto target predicho del modelo 2:  jai reçu ta note #end\n",
            "Texto source:  how can we let this happen\n",
            "Texto target real:  comment pouvonsnous laisser ceci se produire #end\n",
            "Texto target predicho del modelo 1:  comment que que que #end\n",
            "Texto target predicho del modelo 2:  comment pouvonsnous laisser se ceci produire produire #end\n",
            "Texto source:  he asked me if i were happy\n",
            "Texto target real:  il ma demandé si jétais heureux #end\n",
            "Texto target predicho del modelo 1:  il a que que que que #end\n",
            "Texto target predicho del modelo 2:  il ma demandé si jétais content #end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE8QcmlC3xaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e96f98e6-1e2e-4b01-80b6-2d635e306cbe"
      },
      "source": [
        "n_s = 25\n",
        "\n",
        "X_set=X_valid\n",
        "Y_set=Y_valid\n",
        "\n",
        "idx = np.random.choice(np.arange(Y_set.shape[0]), size=n_s, replace=False)\n",
        "Y_set_pred = model.predict_classes(X_set[idx] )\n",
        "Y_set_pred = model2.predict_classes(X_set[idx] )\n",
        "for i, n_sampled in enumerate(idx):\n",
        "    text_input = [idx2word_s[p] for p in predict_words(X_set[n_sampled], data=\"source\")]\n",
        "    print(\"Texto source: \", ' '.join(text_input))\n",
        "    text_real = [idx2word_t[p] for p in predict_words(Y_set[n_sampled,:,0], data=\"target\")]\n",
        "    print(\"Texto target real: \", ' '.join( text_real))\n",
        "    text_sampled = [idx2word_t[p] for p in predict_words(Y_set_pred[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 1: \", ' '.join(text_sampled))\n",
        "    text_sampled2 = [idx2word_t[p] for p in predict_words(Y_set_pred2[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 2: \", ' '.join(text_sampled2))\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto source:  since we the house weve saved a lot on heating oil\n",
            "Texto target real:  depuis que nous avons isolé la maison nous avons beaucoup économisé sur le #end\n",
            "Texto target predicho del modelo 1:  nous nous avons avons de nous avons sommes avons de de nous avons la la de de #end\n",
            "Texto target predicho del modelo 2:  pourraistu je vous prie prie prie prie prie prie prie prie #end\n",
            "Texto source:  perhaps i could come back tomorrow\n",
            "Texto target real:  je pourrais peutêtre revenir demain #end\n",
            "Texto target predicho del modelo 1:  peutêtre peutêtre je je demain demain #end\n",
            "Texto target predicho del modelo 2:  vous avez dit que vous vous allais en en #end\n",
            "Texto source:  youre famous\n",
            "Texto target real:  tu es connue #end\n",
            "Texto target predicho del modelo 1:  vous êtes #end\n",
            "Texto target predicho del modelo 2:  je pense quil est temps que moi commence à à à le dîner dîner dîner dîner dîner dîner dîner dîner #end\n",
            "Texto source:  shes been practicing piano for a year and can play the piano somewhat\n",
            "Texto target real:  elle a pratiqué le piano pendant un an et peut y jouer un peu #end\n",
            "Texto target predicho del modelo 1:  elle a depuis piano piano piano piano piano piano piano piano un un un un #end\n",
            "Texto target predicho del modelo 2:  ne touche pas le touche #end\n",
            "Texto source:  how do you know all that\n",
            "Texto target real:  comment savezvous tout cela #end\n",
            "Texto target predicho del modelo 1:  comment saistu tout cela #end\n",
            "Texto target predicho del modelo 2:  tom ne envie envie nulle part #end\n",
            "Texto source:  im from out of town\n",
            "Texto target real:  je viens de lextérieur de la ville #end\n",
            "Texto target predicho del modelo 1:  je suis en en #end\n",
            "Texto target predicho del modelo 2:  je ne lai jamais dit à quiconque #end\n",
            "Texto source:  its an artificial flower\n",
            "Texto target real:  cest une fleur artificielle #end\n",
            "Texto target predicho del modelo 1:  cest une dune fleur #end\n",
            "Texto target predicho del modelo 2:  ne mets pas les chaussures sur la sur #end\n",
            "Texto source:  i work every day but sunday\n",
            "Texto target real:  je travaille tous les jours sauf le dimanche #end\n",
            "Texto target predicho del modelo 1:  je travaille tous les sauf tous les dimanche dimanche #end\n",
            "Texto target predicho del modelo 2:  mary sa sa mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère mère\n",
            "Texto source:  what time do you want me to be here\n",
            "Texto target real:  à quelle heure veuxtu que je sois là #end\n",
            "Texto target predicho del modelo 1:  que quelle heure je je je que #end\n",
            "Texto target predicho del modelo 2:  peuxtu nous prendre en photo #end\n",
            "Texto source:  she doesnt understand you\n",
            "Texto target real:  elle ne te comprend pas #end\n",
            "Texto target predicho del modelo 1:  elle ne vous comprend pas #end\n",
            "Texto target predicho del modelo 2:  soyez plus gentil avec votre sœur #end\n",
            "Texto source:  she stabbed him\n",
            "Texto target real:  elle le #end\n",
            "Texto target predicho del modelo 1:  elle la #end\n",
            "Texto target predicho del modelo 2:  cest gentil #end\n",
            "Texto source:  were shy\n",
            "Texto target real:  nous sommes timides #end\n",
            "Texto target predicho del modelo 1:  nous sommes timide #end\n",
            "Texto target predicho del modelo 2:  donnezmoi un coup à coup #end\n",
            "Texto source:  learning french takes longer than most people think\n",
            "Texto target real:  apprendre le français requiert plus de temps que ce que la plupart des gens pense #end\n",
            "Texto target predicho del modelo 1:  le que le plus plus plus que que que les #end\n",
            "Texto target predicho del modelo 2:  essaie de ne pas les les #end\n",
            "Texto source:  i wonder why no one tells the truth\n",
            "Texto target real:  je me demande pourquoi personne ne dit la vérité #end\n",
            "Texto target predicho del modelo 1:  je me demande pourquoi personne personne a la la vérité vérité vérité #end\n",
            "Texto target predicho del modelo 2:  vous êtes égoïste #end\n",
            "Texto source:  where have you been for the past three months\n",
            "Texto target real:  où étaistu passé ces trois derniers mois #end\n",
            "Texto target predicho del modelo 1:  où astu fait trois trois trois trois trois trois trois trois #end\n",
            "Texto target predicho del modelo 2:  ils ont commencé commencé travailler travailler #end\n",
            "Texto source:  he isnt any older than i thought\n",
            "Texto target real:  il nest pas plus vieux que ce que je pensais #end\n",
            "Texto target predicho del modelo 1:  il nest pas plus plus que je ne le pensais #end\n",
            "Texto target predicho del modelo 2:  ce pont est de pierre #end\n",
            "Texto source:  tom is now very angry with me\n",
            "Texto target real:  tom est désormais très fâché contre moi #end\n",
            "Texto target predicho del modelo 1:  tom est fait en en colère colère moi #end\n",
            "Texto target predicho del modelo 2:  cest un affaire affaire #end\n",
            "Texto source:  he stood by me whenever i was in trouble\n",
            "Texto target real:  il sest tenu à mon côté chaque fois que je me trouvais dans les ennuis #end\n",
            "Texto target predicho del modelo 1:  il me au dans moi moi que je je #end\n",
            "Texto target predicho del modelo 2:  nous allons dici la tempête #end\n",
            "Texto source:  it seems that something has happened\n",
            "Texto target real:  il semble que quelque chose soit survenu #end\n",
            "Texto target predicho del modelo 1:  il semble que quelque quelque chose passé #end\n",
            "Texto target predicho del modelo 2:  ditesmoi pourquoi vous étiez absent hier lécole hier hier #end\n",
            "Texto source:  a man came to see me yesterday\n",
            "Texto target real:  un homme est venu me voir hier #end\n",
            "Texto target predicho del modelo 1:  hier homme hier hier hier voir hier hier hier hier #end\n",
            "Texto target predicho del modelo 2:  tu ne vas pas si #end\n",
            "Texto source:  the man was ashamed of being born poor\n",
            "Texto target real:  lhomme avait honte dêtre né pauvre #end\n",
            "Texto target predicho del modelo 1:  lhomme fut honte de ce fut pauvre #end\n",
            "Texto target predicho del modelo 2:  jai trouvé la photo que tu tu #end\n",
            "Texto source:  he asked for the money\n",
            "Texto target real:  il a demandé largent #end\n",
            "Texto target predicho del modelo 1:  il a demandé largent largent #end\n",
            "Texto target predicho del modelo 2:  puisje aller chez la maison maintenant maintenant #end\n",
            "Texto source:  they didnt see anything\n",
            "Texto target real:  ils nont rien vu #end\n",
            "Texto target predicho del modelo 1:  elles ne rien rien #end\n",
            "Texto target predicho del modelo 2:  elle est de loin fille fille fille fille fille #end\n",
            "Texto source:  this tastes like tea\n",
            "Texto target real:  ça goûte le thé #end\n",
            "Texto target predicho del modelo 1:  ça a le goût du thé #end\n",
            "Texto target predicho del modelo 2:  à quelle heure chez chez chez chez à matin #end\n",
            "Texto source:  no matter how sneaky you are you can never surprise yourself\n",
            "Texto target real:  toute sournoise que tu sois tu ne peux jamais te surprendre toimême #end\n",
            "Texto target predicho del modelo 1:  tout que que tu ne vous jamais jamais jamais jamais vous #end\n",
            "Texto target predicho del modelo 2:  tom sest coupé le doigt sur un un un #end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXU5Yinx3190",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a8ad681-386a-410c-f39a-57433eef144c"
      },
      "source": [
        "n_s = 25\n",
        "\n",
        "X_set=X_test\n",
        "Y_set=Y_test\n",
        "\n",
        "idx = np.random.choice(np.arange(Y_set.shape[0]), size=n_s, replace=False)\n",
        "Y_set_pred = model.predict_classes(X_set[idx] )\n",
        "Y_set_pred2 = model2.predict_classes(X_set[idx] )\n",
        "for i, n_sampled in enumerate(idx):\n",
        "    text_input = [idx2word_s[p] for p in predict_words(X_set[n_sampled], data=\"source\")]\n",
        "    print(\"Texto source: \", ' '.join(text_input))\n",
        "    text_real = [idx2word_t[p] for p in predict_words(Y_set[n_sampled,:,0], data=\"target\")]\n",
        "    print(\"Texto target real: \", ' '.join( text_real))\n",
        "    text_sampled = [idx2word_t[p] for p in predict_words(Y_set_pred[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 1: \", ' '.join(text_sampled))\n",
        "    text_sampled2 = [idx2word_t[p] for p in predict_words(Y_set_pred2[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 2: \", ' '.join(text_sampled2))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto source:  do you remember the day when we met first\n",
            "Texto target real:  tu te souviens de notre première rencontre #end\n",
            "Texto target predicho del modelo 1:  tom que de de de de de de de #end\n",
            "Texto target predicho del modelo 2:  vous es la nous nous nous sommes sommes sommes sommes sommes nous sommes sommes #end\n",
            "Texto source:  i dont think any of us are happy about what happened\n",
            "Texto target real:  je ne pense pas quaucun dentre nous soit heureux de ce qui sest passé #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas pas que que que que de de #end\n",
            "Texto target predicho del modelo 2:  je ne pense pas que nous quiconque qui avons nous dentre nous nous nous #end\n",
            "Texto source:  im glad to be the one who tells you\n",
            "Texto target real:  je me réjouis dêtre celle qui vous lannonce #end\n",
            "Texto target predicho del modelo 1:  je ne pas de de que que #end\n",
            "Texto target predicho del modelo 2:  je me réjouis dêtre celui que vous le dit dit #end\n",
            "Texto source:  i felt my heart beating\n",
            "Texto target real:  senti mon cœur battre #end\n",
            "Texto target predicho del modelo 1:  je suis que de #end\n",
            "Texto target predicho del modelo 2:  jai me mon cœur cœur cœur #end\n",
            "Texto source:  no one is going to hurt you\n",
            "Texto target real:  personne ne va te faire de mal #end\n",
            "Texto target predicho del modelo 1:  je ne pas de #end\n",
            "Texto target predicho del modelo 2:  personne ne va va de de de #end\n",
            "Texto source:  i sometimes wish i could live a quiet retired sort of life but i doubt i could stand it for more than a few days\n",
            "Texto target real:  parfois je souhaiterais vivre une sorte de vie calme et mais je doute que je puisse la supporter plus de quelques jours #end\n",
            "Texto target predicho del modelo 1:  je suis pas de de de que que que que que que que que que que que que #end\n",
            "Texto target predicho del modelo 2:  je pensais de avoir de de je mais mais mais mais mais mais mais mais mais de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
            "Texto source:  im happy that you were able to find a job so quickly\n",
            "Texto target real:  je suis heureuse que tu aies pu trouver un travail si rapidement #end\n",
            "Texto target predicho del modelo 1:  je ne que que que que que de de de #end\n",
            "Texto target predicho del modelo 2:  je suis heureux que vous travail un un travail travail de de travail de travail #end\n",
            "Texto source:  luckily tom had some money i could borrow\n",
            "Texto target real:  heureusement tom avait un peu dargent que jai pu emprunter #end\n",
            "Texto target predicho del modelo 1:  la le est que que que que que #end\n",
            "Texto target predicho del modelo 2:  heureusement tom a eu largent que je emprunter emprunter #end\n",
            "Texto source:  when youve finished reading that book id like to read it\n",
            "Texto target real:  lorsque vous aurez terminé de lire ce livre jaimerais le lire #end\n",
            "Texto target predicho del modelo 1:  tom je que de de de que que que #end\n",
            "Texto target predicho del modelo 2:  lorsque vous auras fini de lire lire lire lire jaimerais lire lire #end\n",
            "Texto source:  how are you connected\n",
            "Texto target real:  comment êtesvous #end\n",
            "Texto target predicho del modelo 1:  comment que #end\n",
            "Texto target predicho del modelo 2:  comment êtesvous #end\n",
            "Texto source:  dont be sad\n",
            "Texto target real:  ne sois pas #end\n",
            "Texto target predicho del modelo 1:  je pas pas #end\n",
            "Texto target predicho del modelo 2:  ne sois pas triste #end\n",
            "Texto source:  what did you think of him\n",
            "Texto target real:  quastu pensé de lui #end\n",
            "Texto target predicho del modelo 1:  comment que que #end\n",
            "Texto target predicho del modelo 2:  que pensezvous de #end\n",
            "Texto source:  he gave me a look\n",
            "Texto target real:  il me jeta un regard sévère #end\n",
            "Texto target predicho del modelo 1:  il a de de #end\n",
            "Texto target predicho del modelo 2:  il me donna un #end\n",
            "Texto source:  i heard you whistling you must be happy\n",
            "Texto target real:  je tai entendue siffler tu dois être heureuse #end\n",
            "Texto target predicho del modelo 1:  je me que que que que que #end\n",
            "Texto target predicho del modelo 2:  je vous ai siffler siffler vous devez être être #end\n",
            "Texto source:  tom is a little nervous\n",
            "Texto target real:  tom est un peu nerveux #end\n",
            "Texto target predicho del modelo 1:  tom est de la #end\n",
            "Texto target predicho del modelo 2:  tom est un peu nerveux #end\n",
            "Texto source:  where would you like to live\n",
            "Texto target real:  où aimeraistu vivre #end\n",
            "Texto target predicho del modelo 1:  comment que #end\n",
            "Texto target predicho del modelo 2:  où aimeriezvous #end\n",
            "Texto source:  they got it\n",
            "Texto target real:  ils lont eu #end\n",
            "Texto target predicho del modelo 1:  il est #end\n",
            "Texto target predicho del modelo 2:  ils lont #end\n",
            "Texto source:  i started a new blog ill do my best to keep it going\n",
            "Texto target real:  jai commencé un nouveau je vais faire de mon mieux pour le faire vivre #end\n",
            "Texto target predicho del modelo 1:  je suis de de de de de que que de #end\n",
            "Texto target predicho del modelo 2:  jai eu à à le travail pour le travail à le #end\n",
            "Texto source:  move over\n",
            "Texto target real:  #end\n",
            "Texto target predicho del modelo 1:  la #end\n",
            "Texto target predicho del modelo 2:  porte #end\n",
            "Texto source:  itll snow today\n",
            "Texto target real:  il va neiger aujourdhui #end\n",
            "Texto target predicho del modelo 1:  cest de #end\n",
            "Texto target predicho del modelo 2:  il neige aujourdhui aujourdhui #end\n",
            "Texto source:  you have a good job\n",
            "Texto target real:  tu as un bon travail #end\n",
            "Texto target predicho del modelo 1:  tu me de de #end\n",
            "Texto target predicho del modelo 2:  vous avez un bon boulot #end\n",
            "Texto source:  i dont really need your help\n",
            "Texto target real:  je nai pas vraiment besoin de votre aide #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas de de #end\n",
            "Texto target predicho del modelo 2:  je nai pas vraiment besoin besoin ton aide aide #end\n",
            "Texto source:  birds build\n",
            "Texto target real:  les oiseaux construisent des #end\n",
            "Texto target predicho del modelo 1:  la est #end\n",
            "Texto target predicho del modelo 2:  les oiseaux #end\n",
            "Texto source:  i wish you had told me the truth\n",
            "Texto target real:  jeusse aimé que vous mayez dit la vérité #end\n",
            "Texto target predicho del modelo 1:  je que que que de #end\n",
            "Texto target predicho del modelo 2:  jaurais aimé que tu mavez dit la vérité #end\n",
            "Texto source:  i dont think i can do this alone\n",
            "Texto target real:  je ne pense pas pouvoir faire ça seul #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas pas que que #end\n",
            "Texto target predicho del modelo 2:  je ne pense pas pouvoir je faire faire tout #end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMpryget3-ph",
        "colab_type": "text"
      },
      "source": [
        "Visualizamos como funciona nuestros modelos para los distintos conjuntos que creamos. \n",
        "Se esperaría que el primer modelo de mejores resultados debido a su menos val_loss, pero revisando  las sintaxis de las oraciones traducidas, el modelo 2 da resultados más cercanos y más coherentes.\n",
        "\n",
        "Cosas a notar, los dos modelos tienden a entregar frases más cortas, esto puede deberse a que la mayoría de las frases no son largas, no más de 10 palabras. \n",
        "\n",
        "Aunque siendo sincero, la mayoría de las frases traducidas estan lejos de tener el sentido original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ8vIxA7YzkT",
        "colab_type": "text"
      },
      "source": [
        "> i) Realice algún cambio esperando que mejore el modelo entrenado, luego vuelva a visualizar lo predicho por la red *vs* lo real. *Debido a lo costoso en entrenar puede optar por realizar solo un cambio pero que sea significativo*.  Se comentan algunas opciones para utilizar y combinar:\n",
        "* Cambiar  el *embedding* por alguno pre-entrenado\n",
        "* Agregar regularizadores\n",
        "* Asignar peso a las clases/palabras de salida\n",
        "* Cambiar *Global max pooling* por *Average max pooling*\n",
        "* Aumentar o reducir capas\n",
        "* Aumentar o reducir neuronas/unidades "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyOe4eTbYzkU",
        "colab_type": "code",
        "outputId": "c1ba0683-c44b-4ec4-e026-433f743ea6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(input_dim=n_words_s, output_dim=128, input_length=max_inp_length))\n",
        "model3.add(Bidirectional(GRU(256,return_sequences=False)))\n",
        "#model3.add(BatchNormalization()\n",
        "#model3.add(GlobalAveragePooling1D())\n",
        "#model3.add(GlobalMaxPooling1D())\n",
        "#model3.add(RepeatVector(max_out_length))\n",
        "model3.add(RepeatVector(max_out_length))\n",
        "model3.add(Bidirectional(GRU(256,return_sequences=True)))\n",
        "model3.add(TimeDistributed(Dense(n_words_t,activation='softmax')))\n",
        "\n",
        "\n",
        "\n",
        "model3.compile(optimizer = 'rmsprop', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')#,metrics = ['accuracy'])\n",
        "model3.fit(X_train, Y_train, epochs=10, batch_size=256, validation_data=(X_valid, Y_valid), sample_weight = sample_weight)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 147130 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "147130/147130 [==============================] - 284s 2ms/step - loss: 5.4201 - val_loss: 13.6788\n",
            "Epoch 2/10\n",
            "147130/147130 [==============================] - 281s 2ms/step - loss: 4.3973 - val_loss: 13.8905\n",
            "Epoch 3/10\n",
            "147130/147130 [==============================] - 281s 2ms/step - loss: 3.8231 - val_loss: 13.9704\n",
            "Epoch 4/10\n",
            "147130/147130 [==============================] - 284s 2ms/step - loss: 3.4524 - val_loss: 13.9461\n",
            "Epoch 5/10\n",
            "147130/147130 [==============================] - 285s 2ms/step - loss: 3.1882 - val_loss: 13.7735\n",
            "Epoch 6/10\n",
            "147130/147130 [==============================] - 282s 2ms/step - loss: 2.9871 - val_loss: 13.8336\n",
            "Epoch 7/10\n",
            "147130/147130 [==============================] - 281s 2ms/step - loss: 2.8240 - val_loss: 13.8901\n",
            "Epoch 8/10\n",
            "147130/147130 [==============================] - 282s 2ms/step - loss: 2.7266 - val_loss: 13.9565\n",
            "Epoch 9/10\n",
            "147130/147130 [==============================] - 281s 2ms/step - loss: 2.5767 - val_loss: 13.9293\n",
            "Epoch 10/10\n",
            "147130/147130 [==============================] - 281s 2ms/step - loss: 2.5079 - val_loss: 14.0010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7998ed0f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEBwwu5XvZR",
        "colab_type": "text"
      },
      "source": [
        "Creamos un modelo que utiliza las capas bidireccionales utilizadas en las preguntas anteriores, de esta manera creemos que se  puede lograr un mejor aprendizaje tanto en el encoder como en el decoder.\n",
        "Este cambio en las capas da como resultado una loss y una val_losss menor a la de los otros dos modelos pero con un costo computacional mayor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjbCRQFWH6Bs",
        "colab_type": "code",
        "outputId": "0e4ea86d-7174-43e1-9ce8-e18238945dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 44, 128)           939520    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 512)               591360    \n",
            "_________________________________________________________________\n",
            "repeat_vector_11 (RepeatVect (None, 54, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 54, 512)           1181184   \n",
            "_________________________________________________________________\n",
            "time_distributed_11 (TimeDis (None, 54, 12339)         6329907   \n",
            "=================================================================\n",
            "Total params: 9,041,971\n",
            "Trainable params: 9,041,971\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LN1COD696TU",
        "colab_type": "code",
        "outputId": "9e15bd2b-29a7-4e72-c054-a30fe84e45b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_s = 50\n",
        "Y_set=Y_train\n",
        "X_set=X_train\n",
        "\n",
        "idx = np.random.choice(np.arange(Y_set.shape[0]), size=n_s, replace=False)\n",
        "Y_set_pred3 = model3.predict_classes(X_set[idx] )\n",
        "Y_set_pred = model.predict_classes(X_set[idx] )\n",
        "Y_set_pred2 = model2.predict_classes(X_set[idx] )\n",
        "\n",
        "for i, n_sampled in enumerate(idx):\n",
        "    text_input = [idx2word_s[p] for p in predict_words(X_set[n_sampled], data=\"source\")]\n",
        "    print(\"Texto source: \", ' '.join(text_input))\n",
        "    text_real = [idx2word_t[p] for p in predict_words(Y_set[n_sampled,:,0], data=\"target\")]\n",
        "    print(\"Texto target real: \", ' '.join( text_real))\n",
        "    text_sampled = [idx2word_t[p] for p in predict_words(Y_set_pred[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 1: \", ' '.join(text_sampled))\n",
        "    text_sampled2 = [idx2word_t[p] for p in predict_words(Y_set_pred2[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 2: \", ' '.join(text_sampled2))\n",
        "    text_sampled3 = [idx2word_t[p] for p in predict_words(Y_set_pred3[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 3: \", ' '.join(text_sampled3))\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto source:  a of sweat started on his brow\n",
            "Texto target real:  une goutte de sueur sur son #end\n",
            "Texto target predicho:  une de de sur sur sur #end\n",
            "Texto source:  we gave it to the man\n",
            "Texto target real:  nous lavons donné au monsieur #end\n",
            "Texto target predicho:  nous lavons le donné un #end\n",
            "Texto source:  i sleep with two in the winter\n",
            "Texto target real:  je dors avec deux en hiver #end\n",
            "Texto target predicho:  je suis dans dans deux #end\n",
            "Texto source:  what part is wrong\n",
            "Texto target real:  quelle partie est mauvaise #end\n",
            "Texto target predicho:  quelle quelle partie #end\n",
            "Texto source:  tom leaned over the bridge\n",
            "Texto target real:  tom se pencha pardessus le pont #end\n",
            "Texto target predicho:  tom se au le le pont #end\n",
            "Texto source:  i believe in my abilities\n",
            "Texto target real:  jai foi en mes #end\n",
            "Texto target predicho:  je crois en mes #end\n",
            "Texto source:  she became scared when she noticed the man following her\n",
            "Texto target real:  elle commença à avoir peur lorsquelle remarqua lhomme qui la suivait #end\n",
            "Texto target predicho:  elle a presque comme quelle le le #end\n",
            "Texto source:  are you trying to impress me\n",
            "Texto target real:  essayezvous de #end\n",
            "Texto target predicho:  estu de train #end\n",
            "Texto source:  i would like a large slice of cake and a cup of coffee\n",
            "Texto target real:  jaimerais bien un gros morceau de gâteau et une tasse de café #end\n",
            "Texto target predicho:  je un une de de et et de de de café #end\n",
            "Texto source:  please turn off your engine\n",
            "Texto target real:  coupe ton moteur sil te plait #end\n",
            "Texto target predicho:  je ton ton sil te plait #end\n",
            "Texto source:  youre here early\n",
            "Texto target real:  tu viens tôt #end\n",
            "Texto target predicho:  vous êtes tôt #end\n",
            "Texto source:  i wasnt ready for what was about to happen\n",
            "Texto target real:  je nétais pas prête pour ce qui allait arriver #end\n",
            "Texto target predicho:  je nétais pas nécessaire à ce qui qui se #end\n",
            "Texto source:  thats a pretty dress you have on\n",
            "Texto target real:  cest une jolie robe que tu portes #end\n",
            "Texto target predicho:  cest une robe de tu vous #end\n",
            "Texto source:  i just thought you were happy\n",
            "Texto target real:  jai simplement pensé que vous étiez heureuses #end\n",
            "Texto target predicho:  jai simplement pensé que vous étais heureuses #end\n",
            "Texto source:  i never feed my dog raw meat\n",
            "Texto target real:  je ne donne jamais de viande crue à manger à mon chien #end\n",
            "Texto target predicho:  je ne jamais jamais mon de de la de #end\n",
            "Texto source:  prices are about to go up again\n",
            "Texto target real:  les prix vont encore augmenter #end\n",
            "Texto target predicho:  les prix sont à de de #end\n",
            "Texto source:  loosen up\n",
            "Texto target real:  aller #end\n",
            "Texto target predicho:  #end\n",
            "Texto source:  pets are allowed\n",
            "Texto target real:  les animaux sont autorisés #end\n",
            "Texto target predicho:  les animaux sont sont #end\n",
            "Texto source:  in what month were you born\n",
            "Texto target real:  quel mois estu né #end\n",
            "Texto target predicho:  en quelle heure estu #end\n",
            "Texto source:  were friends now\n",
            "Texto target real:  nous sommes désormais amis #end\n",
            "Texto target predicho:  nous sommes désormais amis #end\n",
            "Texto source:  theres a restaurant pretty close to here but its not very good\n",
            "Texto target real:  il y a un restaurant assez près dici mais il nest pas très bon #end\n",
            "Texto target predicho:  il y a un bon mais mais mais nest nest pas pas pas #end\n",
            "Texto source:  i dont think thats legal\n",
            "Texto target real:  je ne pense pas que cela soit légal #end\n",
            "Texto target predicho:  je ne pense pas que soit soit #end\n",
            "Texto source:  it doesnt happen very often\n",
            "Texto target real:  ça ne se produit pas très souvent #end\n",
            "Texto target predicho:  ça ne se pas pas souvent #end\n",
            "Texto source:  i guess you think youre pretty special dont you\n",
            "Texto target real:  je présume que tu penses être plutôt nestce pas #end\n",
            "Texto target predicho:  je suppose que vous penses pas plutôt nestce nestce pas #end\n",
            "Texto source:  are you already married\n",
            "Texto target real:  estu déjà mariée #end\n",
            "Texto target predicho:  êtesvous déjà mariée #end\n",
            "Texto source:  i havent seen you in ages\n",
            "Texto target real:  je ne tai pas vu depuis des lustres #end\n",
            "Texto target predicho:  je ne vous ai pas depuis des des #end\n",
            "Texto source:  he begged for his life\n",
            "Texto target real:  il a quon épargne sa vie #end\n",
            "Texto target predicho:  il a de sa vie #end\n",
            "Texto source:  people on that island are very poor\n",
            "Texto target real:  les gens sur cette île sont très pauvres #end\n",
            "Texto target predicho:  les gens des sont sont très très #end\n",
            "Texto source:  trust me you dont want to know\n",
            "Texto target real:  faitesmoi confiance vous ne voulez pas savoir #end\n",
            "Texto target predicho:  dismoi de ne ne veux pas #end\n",
            "Texto source:  im free every day but monday\n",
            "Texto target real:  je suis libre tous les jours sauf le lundi #end\n",
            "Texto target predicho:  je suis libre jours les où #end\n",
            "Texto source:  thats why tom and i were disappointed\n",
            "Texto target real:  cest pourquoi tom et moi étions déçus #end\n",
            "Texto target predicho:  cest pourquoi tom et tom suis #end\n",
            "Texto source:  there is no rule without exceptions\n",
            "Texto target real:  il ny a pas de règle sans exception #end\n",
            "Texto target predicho:  il ny a pas de sans #end\n",
            "Texto source:  it is often said that the japanese are hard workers\n",
            "Texto target real:  on entend souvent dire que les japonais sont des bons travailleurs #end\n",
            "Texto target predicho:  on le souvent que que souvent est est des #end\n",
            "Texto source:  what fruit do you like best\n",
            "Texto target real:  quel fruit estce que tu préfères #end\n",
            "Texto target predicho:  quels fruit préférezvous #end\n",
            "Texto source:  toms doctor told him to quit smoking\n",
            "Texto target real:  le médecin de tom lui a dit darrêter de fumer #end\n",
            "Texto target predicho:  le médecin de tom de de de fumer de #end\n",
            "Texto source:  do you have health insurance\n",
            "Texto target real:  êtesvous couvertes par une assurance médicale #end\n",
            "Texto target predicho:  êtesvous une médicale médicale médicale médicale #end\n",
            "Texto source:  youre just like your father\n",
            "Texto target real:  tu es exactement comme ton père #end\n",
            "Texto target predicho:  tu es exactement de ton père #end\n",
            "Texto source:  his remark has nothing to do with the subject\n",
            "Texto target real:  sa remarque rien à faire avec le sujet #end\n",
            "Texto target predicho:  sa na na rien rien faire faire #end\n",
            "Texto source:  ive lived a long life\n",
            "Texto target real:  jai vécu une longue vie #end\n",
            "Texto target predicho:  jai vécu une vie #end\n",
            "Texto source:  he died of lung cancer\n",
            "Texto target real:  il est mort dun cancer du poumon #end\n",
            "Texto target predicho:  il est mort dun cancer #end\n",
            "Texto source:  my plan is working perfectly\n",
            "Texto target real:  mon plan se déroule à la perfection #end\n",
            "Texto target predicho:  mon plan va à #end\n",
            "Texto source:  who phoned\n",
            "Texto target real:  qui estce qui a téléphoné #end\n",
            "Texto target predicho:  qui a #end\n",
            "Texto source:  what are you doing in the attic\n",
            "Texto target real:  que faistu dans le grenier #end\n",
            "Texto target predicho:  que faistu dans le #end\n",
            "Texto source:  i think youre a liar\n",
            "Texto target real:  je pense que vous êtes un menteur #end\n",
            "Texto target predicho:  je pense que tu es un menteur #end\n",
            "Texto source:  i can show you a better time\n",
            "Texto target real:  je sais mieux vous divertir #end\n",
            "Texto target predicho:  je peux vous vous un moment moment #end\n",
            "Texto source:  do you like the way you look\n",
            "Texto target real:  aimestu ton apparence #end\n",
            "Texto target predicho:  aimestu la apparence #end\n",
            "Texto source:  i want to be more than friends\n",
            "Texto target real:  je veux être davantage quun ami #end\n",
            "Texto target predicho:  je veux être plus que #end\n",
            "Texto source:  why did you call\n",
            "Texto target real:  pourquoi astu appelé #end\n",
            "Texto target predicho:  pourquoi avezvous appelé #end\n",
            "Texto source:  lets just try it\n",
            "Texto target real:  essayons #end\n",
            "Texto target predicho:  essayons de #end\n",
            "Texto source:  do you know if she can speak english\n",
            "Texto target real:  saistu si elle sait parler langlais #end\n",
            "Texto target predicho:  savezvous parler parler parler parler #end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4jCDJBu99Rx",
        "colab_type": "code",
        "outputId": "8079df5b-f292-49f8-ab11-9618e5302518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_s = 25\n",
        "Y_set=Y_valid\n",
        "X_set=X_valid\n",
        "\n",
        "idx = np.random.choice(np.arange(Y_set.shape[0]), size=n_s, replace=False)\n",
        "Y_set_pred3 = model3.predict_classes(X_set[idx] )\n",
        "Y_set_pred = model.predict_classes(X_set[idx] )\n",
        "Y_set_pred2 = model2.predict_classes(X_set[idx] )\n",
        "\n",
        "for i, n_sampled in enumerate(idx):\n",
        "    text_input = [idx2word_s[p] for p in predict_words(X_set[n_sampled], data=\"source\")]\n",
        "    print(\"Texto source: \", ' '.join(text_input))\n",
        "    text_real = [idx2word_t[p] for p in predict_words(Y_set[n_sampled,:,0], data=\"target\")]\n",
        "    print(\"Texto target real: \", ' '.join( text_real))\n",
        "    text_sampled = [idx2word_t[p] for p in predict_words(Y_set_pred[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 1: \", ' '.join(text_sampled))\n",
        "    text_sampled2 = [idx2word_t[p] for p in predict_words(Y_set_pred2[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 2: \", ' '.join(text_sampled2))\n",
        "    text_sampled3 = [idx2word_t[p] for p in predict_words(Y_set_pred3[i], data=\"target\")]\n",
        "    print(\"Texto target predicho: \", ' '.join(text_sampled3))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto source:  i didnt want to tell you this\n",
            "Texto target real:  je ne voulais pas te dire ça #end\n",
            "Texto target predicho:  je ne voulais pas te dire ça #end\n",
            "Texto source:  give the money to my son\n",
            "Texto target real:  donnez largent à mon fils #end\n",
            "Texto target predicho:  donne mon mon argent mon mon #end\n",
            "Texto source:  i thought you were smart\n",
            "Texto target real:  je pensais que tu étais intelligent #end\n",
            "Texto target predicho:  je pensais que vous étais #end\n",
            "Texto source:  whats up\n",
            "Texto target real:  quoi de beau #end\n",
            "Texto target predicho:  cest quoi #end\n",
            "Texto source:  i could use some advice\n",
            "Texto target real:  un conseil ne serait pas de refus #end\n",
            "Texto target predicho:  je pourrais donner quelques #end\n",
            "Texto source:  did we miss anything\n",
            "Texto target real:  avonsnous manqué quelque chose #end\n",
            "Texto target predicho:  avonsnous avonsnous quelque chose #end\n",
            "Texto source:  the floor gave way\n",
            "Texto target real:  le sol céda #end\n",
            "Texto target predicho:  le chemin a pris #end\n",
            "Texto source:  she has been to hawaii several times\n",
            "Texto target real:  elle est allée à hawaii plusieurs fois #end\n",
            "Texto target predicho:  elle a déjà à plusieurs plusieurs #end\n",
            "Texto source:  tom and mary got back together after a trial\n",
            "Texto target real:  tom et marie se ensemble après une difficile #end\n",
            "Texto target predicho:  tom et mary se avoir un le le le #end\n",
            "Texto source:  im glad you realize that\n",
            "Texto target real:  je me réjouis que vous en preniez conscience #end\n",
            "Texto target predicho:  je suis content que vous aies daccord ça #end\n",
            "Texto source:  i phoned\n",
            "Texto target real:  jai téléphoné #end\n",
            "Texto target predicho:  je #end\n",
            "Texto source:  i was astonished\n",
            "Texto target real:  je fus étonné #end\n",
            "Texto target predicho:  jétais été #end\n",
            "Texto source:  i just changed my mind thats all\n",
            "Texto target real:  jai simplement changé davis cest tout #end\n",
            "Texto target predicho:  je simplement de que tout changé #end\n",
            "Texto source:  how many days are there in a leap year\n",
            "Texto target real:  combien de jours compte une année bissextile #end\n",
            "Texto target predicho:  combien de jours de jours dans année année #end\n",
            "Texto source:  are you finished reading the newspaper\n",
            "Texto target real:  astu fini de lire le journal #end\n",
            "Texto target predicho:  avezvous fini de lire le journal #end\n",
            "Texto source:  the door doesnt lock\n",
            "Texto target real:  la porte ne se ferme pas #end\n",
            "Texto target predicho:  la porte ne pas pas #end\n",
            "Texto source:  you shouldve turned left\n",
            "Texto target real:  tu aurais dû tourner à gauche #end\n",
            "Texto target predicho:  tu auriez dû appeler #end\n",
            "Texto source:  what newspaper do you subscribe to\n",
            "Texto target real:  à quel journal êtesvous #end\n",
            "Texto target predicho:  à quel journal estu #end\n",
            "Texto source:  lets all remember to be nice\n",
            "Texto target real:  tous dêtre gentils #end\n",
            "Texto target predicho:  tous dêtre tous dêtre être #end\n",
            "Texto source:  we cant keep on fooling ourselves\n",
            "Texto target real:  nous ne pouvons pas continuer à nous abuser #end\n",
            "Texto target predicho:  nous ne pouvons pas le nous #end\n",
            "Texto source:  dont talk about work were on vacation\n",
            "Texto target real:  ne parlez pas de travail nous sommes en vacances #end\n",
            "Texto target predicho:  ne parle pas de travail à enfants enfants #end\n",
            "Texto source:  enjoy yourself for a change\n",
            "Texto target real:  amusezvous pour changer #end\n",
            "Texto target predicho:  merci pour pour le #end\n",
            "Texto source:  philosophy is often regarded as difficult\n",
            "Texto target real:  la philosophie est considérée comme difficile #end\n",
            "Texto target predicho:  le souvent est souvent souvent souvent souvent #end\n",
            "Texto source:  exercise is to the body what thinking is to the brain\n",
            "Texto target real:  le sport est au corps ce que la pensée est au cerveau #end\n",
            "Texto target predicho:  le est à à à à est est est à #end\n",
            "Texto source:  tom called mary but the line was busy\n",
            "Texto target real:  tom a appelé mary mais la était occupée #end\n",
            "Texto target predicho:  tom a mary mary mary mary le le #end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cbFfXJk-BQv",
        "colab_type": "code",
        "outputId": "e96fe12c-e0fe-4457-a961-eb5f15e93200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_s = 25\n",
        "Y_set=Y_test\n",
        "X_set=X_test\n",
        "\n",
        "idx = np.random.choice(np.arange(Y_set.shape[0]), size=n_s, replace=False)\n",
        "Y_set_pred3 = model3.predict_classes(X_set[idx] )\n",
        "Y_set_pred = model.predict_classes(X_set[idx] )\n",
        "Y_set_pred2 = model2.predict_classes(X_set[idx] )\n",
        "\n",
        "\n",
        "for i, n_sampled in enumerate(idx):\n",
        "    text_input = [idx2word_s[p] for p in predict_words(X_set[n_sampled], data=\"source\")]\n",
        "    print(\"Texto source: \", ' '.join(text_input))\n",
        "    text_real = [idx2word_t[p] for p in predict_words(Y_set[n_sampled,:,0], data=\"target\")]\n",
        "    print(\"Texto target real: \", ' '.join( text_real))\n",
        "    text_sampled = [idx2word_t[p] for p in predict_words(Y_set_pred[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 1: \", ' '.join(text_sampled))\n",
        "    text_sampled2 = [idx2word_t[p] for p in predict_words(Y_set_pred2[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 2: \", ' '.join(text_sampled2))\n",
        "    text_sampled3 = [idx2word_t[p] for p in predict_words(Y_set_pred3[i], data=\"target\")]\n",
        "    print(\"Texto target predicho del modelo 3\", ' '.join(text_sampled3))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto source:  nobody can whatll happen\n",
            "Texto target real:  nul ne peut prévoir ce qui va arriver #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas #end\n",
            "Texto target predicho del modelo 2:  personne ne peut à se arriver #end\n",
            "Texto target predicho: del modelo 3 personne ne peut ce qui #end\n",
            "Texto source:  although she is rich she dresses quite simply\n",
            "Texto target real:  bien quelle soit riche elle se très simplement #end\n",
            "Texto target predicho del modelo 1:  cest est est est de de #end\n",
            "Texto target predicho del modelo 2:  bien bien quelle elle elle plus plus plus elle #end\n",
            "Texto target predicho: del modelo 3 en quelle elle elle elle elle elle #end\n",
            "Texto source:  the bell has not rung yet\n",
            "Texto target real:  la cloche na pas encore sonné #end\n",
            "Texto target predicho del modelo 1:  cest que ne pas #end\n",
            "Texto target predicho del modelo 2:  la chose na pas encore encore #end\n",
            "Texto target predicho: del modelo 3 la question nest pas encore #end\n",
            "Texto source:  we have a deadline\n",
            "Texto target real:  nous avons une #end\n",
            "Texto target predicho del modelo 1:  il a de la #end\n",
            "Texto target predicho del modelo 2:  nous avons une date #end\n",
            "Texto target predicho: del modelo 3 nous avons une #end\n",
            "Texto source:  this invention will make you very rich\n",
            "Texto target real:  cette invention te rendra très riche #end\n",
            "Texto target predicho del modelo 1:  cest est que de #end\n",
            "Texto target predicho del modelo 2:  cette vous te riche très riche #end\n",
            "Texto target predicho: del modelo 3 cette aussi est très très #end\n",
            "Texto source:  ill never be as good as you\n",
            "Texto target real:  je ne serai jamais aussi bonne que vous #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas pas que #end\n",
            "Texto target predicho del modelo 2:  je ne serai jamais aussi bon que toi #end\n",
            "Texto target predicho: del modelo 3 je ne serai jamais aussi comme que #end\n",
            "Texto source:  we dont make mistakes\n",
            "Texto target real:  nous ne pas derreurs #end\n",
            "Texto target predicho del modelo 1:  il ne pas pas #end\n",
            "Texto target predicho del modelo 2:  nous ne sommes des erreurs #end\n",
            "Texto target predicho: del modelo 3 nous navons pas des erreurs #end\n",
            "Texto source:  i really appreciate you meeting with me\n",
            "Texto target real:  japprécie vraiment que tu me rencontres #end\n",
            "Texto target predicho del modelo 1:  je suis pas de de #end\n",
            "Texto target predicho del modelo 2:  japprécie vraiment que vous vous suis avec moi avec #end\n",
            "Texto target predicho: del modelo 3 je vraiment que vous me sois à à moi #end\n",
            "Texto source:  dont tell me let me guess\n",
            "Texto target real:  ne me dis pas laissemoi le deviner #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas pas #end\n",
            "Texto target predicho del modelo 2:  ne me le dis pas laissezmoi deviner deviner #end\n",
            "Texto target predicho: del modelo 3 ne me dites pas pas me deviner #end\n",
            "Texto source:  he had no\n",
            "Texto target real:  il est dépourvu de signes #end\n",
            "Texto target predicho del modelo 1:  il ne pas #end\n",
            "Texto target predicho del modelo 2:  il navait pas #end\n",
            "Texto target predicho: del modelo 3 il na pas #end\n",
            "Texto source:  thats the way he likes it\n",
            "Texto target real:  cest comme ça quil lapprécie #end\n",
            "Texto target predicho del modelo 1:  il est est de #end\n",
            "Texto target predicho del modelo 2:  cest le façon dont laime laime #end\n",
            "Texto target predicho: del modelo 3 cest la quil le laime #end\n",
            "Texto source:  dont turn around\n",
            "Texto target real:  ne vous retournez pas #end\n",
            "Texto target predicho del modelo 1:  je pas pas #end\n",
            "Texto target predicho del modelo 2:  ne te pas pas #end\n",
            "Texto target predicho: del modelo 3 ne pas pas #end\n",
            "Texto source:  i just got up give me a few minutes to get ready\n",
            "Texto target real:  je viens de me lever donnemoi quelques minutes pour me préparer #end\n",
            "Texto target predicho del modelo 1:  je suis pas de de de de #end\n",
            "Texto target predicho del modelo 2:  je viens de me me me me me quelques quelques #end\n",
            "Texto target predicho: del modelo 3 je viens suis de minutes minutes minutes minutes minutes me #end\n",
            "Texto source:  i have all the money i need\n",
            "Texto target real:  jai toute la thune quil me faut #end\n",
            "Texto target predicho del modelo 1:  je a de de que que #end\n",
            "Texto target predicho del modelo 2:  jai le tout dont dont jai besoin besoin besoin #end\n",
            "Texto target predicho: del modelo 3 jai tout tout dont dont javais besoin #end\n",
            "Texto source:  tom got the ticket for free\n",
            "Texto target real:  tom a eu le ticket gratuitement #end\n",
            "Texto target predicho del modelo 1:  tom est de de #end\n",
            "Texto target predicho del modelo 2:  tom a eu le billet pour #end\n",
            "Texto target predicho: del modelo 3 tom a pris le billet #end\n",
            "Texto source:  tom will be here by evening\n",
            "Texto target real:  tom sera présent ce soir #end\n",
            "Texto target predicho del modelo 1:  tom que de de #end\n",
            "Texto target predicho del modelo 2:  tom sera là à soir #end\n",
            "Texto target predicho: del modelo 3 tom sera là dans soir #end\n",
            "Texto source:  you were drunk last night werent you\n",
            "Texto target real:  vous étiez la nuit passée pas vrai #end\n",
            "Texto target predicho del modelo 1:  jai suis de de de #end\n",
            "Texto target predicho del modelo 2:  vous étiez dernière la nuit dernière pas dernière #end\n",
            "Texto target predicho: del modelo 3 vous étiez hier la nuit nestce pas #end\n",
            "Texto source:  dont tell me what i can and cant do\n",
            "Texto target real:  ne me dites pas ce que je peux ou ne peux faire #end\n",
            "Texto target predicho del modelo 1:  je ne pas pas pas que pas pas pas pas pas #end\n",
            "Texto target predicho del modelo 2:  ne me dis pas à ce ce je peux peux peux peux peux #end\n",
            "Texto target predicho: del modelo 3 ne me pas à ce que je peux peux peux faire faire peux #end\n",
            "Texto source:  dont try to do too much\n",
            "Texto target real:  nessayez pas de trop en faire #end\n",
            "Texto target predicho del modelo 1:  je ne pas de #end\n",
            "Texto target predicho del modelo 2:  ne pas de trop #end\n",
            "Texto target predicho: del modelo 3 ne pas de trop #end\n",
            "Texto source:  did you know there was a secret passage\n",
            "Texto target real:  savaistu quil y avait un passage secret #end\n",
            "Texto target predicho del modelo 1:  tom que que de de #end\n",
            "Texto target predicho del modelo 2:  avezvous atil que y a un un un #end\n",
            "Texto target predicho: del modelo 3 avezvous que que y un un #end\n",
            "Texto source:  it sounds like a fun job\n",
            "Texto target real:  ça semble être un travail marrant #end\n",
            "Texto target predicho del modelo 1:  il que de de de #end\n",
            "Texto target predicho del modelo 2:  on a lair dun un boulot boulot #end\n",
            "Texto target predicho: del modelo 3 ça semble lair un un bon #end\n",
            "Texto source:  as he gets older hes getting more and more stubborn\n",
            "Texto target real:  en vieillissant il devient de plus en plus têtu #end\n",
            "Texto target predicho del modelo 1:  il que que que de de de de de #end\n",
            "Texto target predicho del modelo 2:  en plus en plus plus en et de #end\n",
            "Texto target predicho: del modelo 3 il il il il il plus plus plus #end\n",
            "Texto source:  it might rain before evening\n",
            "Texto target real:  il se peut quil pleuve avant ce soir #end\n",
            "Texto target predicho del modelo 1:  cest que que de #end\n",
            "Texto target predicho del modelo 2:  il pourrait pourrait pleuvoir ce soir soir soir soir #end\n",
            "Texto target predicho: del modelo 3 il se pleuvoir pleuvoir pleuvoir #end\n",
            "Texto source:  do you want to go to the station with me\n",
            "Texto target real:  voulezvous venir avec moi jusquà la gare #end\n",
            "Texto target predicho del modelo 1:  tom que de de de #end\n",
            "Texto target predicho del modelo 2:  veuxtu aller à la gare moi gare #end\n",
            "Texto target predicho: del modelo 3 voulezvous me à à la moi #end\n",
            "Texto source:  my father stopped drinking\n",
            "Texto target real:  mon père a cessé de boire #end\n",
            "Texto target predicho del modelo 1:  la est a de #end\n",
            "Texto target predicho del modelo 2:  mon père a arrêté de boire #end\n",
            "Texto target predicho: del modelo 3 mon père a de de boire #end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXK6YPLU1PQN",
        "colab_type": "text"
      },
      "source": [
        "Tanto este como los otros modelos empiezan a fallar cuando se pide que tradusca las palabras poco comunes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMiNXT3wYzkc",
        "colab_type": "text"
      },
      "source": [
        "> j) A pesar de que la tarea de medir qué tan similar es un texto a otro ya es un área de investigación propia [[6]](#refs), usted deberá utilizar alguna métrica de desempeño para ver qué tan buena es la traducción del texto *versus* el texto real entregado. Debido a que la métrica de *Exact Matching* (EM) puede ser muy drástica, mida *f1 score* por texto además de proponer alguna otra técnica de evaluación para medir sobre el conjunto de pruebas y los otros conjuntos si estima conveniente. Puede basarse en otros trabajos como *Image captioning* o *Text summary*. \n",
        "*Hint: Debido a los problemas de memoria al realizar un forward-pass, solo seleccione un subconjunto $N_{sub}$ del conjunto de pruebas para realizar ésta evaluación, se aconseja entre 1000 y 5000.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-dfTPwTYzkh",
        "colab_type": "text"
      },
      "source": [
        ">> La función de *f1 score* en este extracto se calcula en base al *precision* y *recall* de que aparezca cada una de las palabras predichas dentro de las palabras reales (como si cada palabra fuera una clase de \"aparece\" o no), **sin importar el orden ni la ocurrencia**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68w29jpP_hHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "m = MultiLabelBinarizer().fit([np.arange(n_words_t)]) \n",
        "def calculate_f1(true, pred):\n",
        "    true = np.squeeze(true)\n",
        "    pred = np.squeeze(pred)\n",
        "    binarized_true = m.transform([predict_words(true)])[0] #onehot of words appear\n",
        "    binarized_pred = m.transform([predict_words(pred)])[0] #onehot of words appear\n",
        "    return f1_score(binarized_true, binarized_pred, average='binary') #only on appearing words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSswx4FV-026",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 1000\n",
        "Y_set=Y_test\n",
        "X_set=X_test\n",
        "idx = np.random.choice(np.arange(Y_set.shape[0]), size=n, replace=False)\n",
        "Y_set_pred = model.predict_classes(X_set[idx] )\n",
        "Y_set_pred2 = model2.predict_classes(X_set[idx] )\n",
        "Y_set_pred3 = model3.predict_classes(X_set[idx] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbqGJIKjYzkc",
        "colab_type": "code",
        "outputId": "b187512c-8b46-4b5f-9d49-764ebc95e5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "f1_final = np.mean([calculate_f1(true_words,pred_words) for true_words,pred_words in zip(Y_set[idx],Y_set_pred)])\n",
        "f1_final*100 #porcentaje\n",
        "f1_final2 = np.mean([calculate_f1(true_words,pred_words) for true_words,pred_words in zip(Y_set[idx],Y_set_pred2)])\n",
        "f1_final2*100 #porcentaje\n",
        "f1_final3 = np.mean([calculate_f1(true_words,pred_words) for true_words,pred_words in zip(Y_set[idx],Y_set_pred3)])\n",
        "f1_final3*100 #porcentaje\n",
        "\n",
        "print(\"El modelo1 tiene un desempeño f1: \", f1_final*100)\n",
        "print(\"El modelo2 tiene un desempeño f1: \", f1_final2*100)\n",
        "print(\"El modelo3 tiene un desempeño f1: \", f1_final3*100)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El modelo primero tiene un desempeño f1:  36.697076769979255\n",
            "El modelo2 primero tiene un desempeño f1:  67.19417251630827\n",
            "El modelo3 primero tiene un desempeño f1:  64.44605604750812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuSrfiR9Yzke",
        "colab_type": "text"
      },
      "source": [
        "Para el score f1, un desempeño cecarno al 100 es optimo mientras que uno cercano a 0 es deficiente. Según los resultados podemos ver que el modelo 2 tiene un desempeño ligeramente mejor que el de f1, si comparamos las traducciones que obtuvimos anteriormente podemos ver que el segundo modelo tiene un desempeño más estable, normalmente evita dar traducciones completamente descabelladas.\n",
        "\n",
        "Otra metrica que se puede usar es BLEU, pero este modelo tampoco considera la coherencia de la traducción o la gramatica.\n",
        "Una gracia de este modelo es que no le da mas valor a una palabra en exceso repetida, a diferencia de otros modelos que consideran si cada palabra traducida esta en la frase original, aunque las plabara se repitan o no tengan un orden coherente.\n",
        "\n",
        "https://en.wikipedia.org/wiki/BLEU\n",
        "https://stackoverflow.com/questions/32395880/calculate-bleu-score-in-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdyKqKt9_9RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "m = MultiLabelBinarizer().fit([np.arange(n_words_t)]) \n",
        "def calculate_BLEU(true, pred):\n",
        "    true = np.squeeze(true)\n",
        "    pred = np.squeeze(pred)\n",
        "    binarized_true = m.transform([predict_words(true)])[0] #onehot of words appear\n",
        "    binarized_pred = m.transform([predict_words(pred)])[0] #onehot of words appear\n",
        "    return nltk.translate.bleu_score.sentence_bleu([binarized_true], binarized_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKY8KcaQYzkf",
        "colab_type": "code",
        "outputId": "7d995598-d4a1-43bf-873f-2f8dd19b0d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "\n",
        "BLEU_final = np.mean([calculate_BLEU(true_words,pred_words) for true_words,pred_words in zip(Y_set[idx],Y_set_pred)])\n",
        "BLEU_final*100 #porcentaje\n",
        "BLEU_final2 = np.mean([calculate_BLEU(true_words,pred_words) for true_words,pred_words in zip(Y_set[idx],Y_set_pred2)])\n",
        "BLEU_final2*100 #porcentaje\n",
        "BLEU_final3 = np.mean([calculate_BLEU(true_words,pred_words) for true_words,pred_words in zip(Y_set[idx],Y_set_pred3)])\n",
        "BLEU_final3*100 #porcentaje\n",
        "\n",
        "print(\"El modelo1 tiene un desempeño BLEU: \", BLEU_final*100)\n",
        "print(\"El modelo2 tiene un desempeño BLEU: \", BLEU_final2*100)\n",
        "print(\"El modelo3 tiene un desempeño BLEU: \", BLEU_final3*100)\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El modelo primero tiene un desempeño BLEU:  99.93447079907762\n",
            "El modelo2 primero tiene un desempeño BLEU:  99.96932600157862\n",
            "El modelo3 primero tiene un desempeño BLEU:  99.9642615684641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx8m0vcgYzki",
        "colab_type": "text"
      },
      "source": [
        "> k) En ves de volver a variar el modelo de *Encoder*, dejaremos una representación manual explícita (*no entrenable*) a través de extraer características manuales de los textos *source*, como por ejemplo representaciones *term frequency* (TF) o TF-IDF, proporcionadas a través de __[sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)__. Luego, con esto generado, defina y entrene el modelo *Decoder* neuronal como el presentado en las preguntas anteriores, ésto es comenzar desde la capa *RepeatVector* hasta llegar a la clasificación sobre el texto *target*. Compare el desempeño con lo presentado en (j) y lo visualizado en (h)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URAgp81aYzkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def dummy_fun(doc):\n",
        "    return doc\n",
        "tf_idf = TfidfVectorizer(analyzer='word',tokenizer=dummy_fun,preprocessor=dummy_fun,\n",
        "                         token_pattern=None,use_idf= True, smooth_idf=True, norm='l2')   \n",
        "X_train_tfidf = tf_idf.fit_transform(dataX_train).astype('float32').todense()\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "model4 = Sequential()\n",
        "model4.add(RepeatVector(max_out_length)) #conection\n",
        "\n",
        "model4.add(CuDNNGRU(128, return_sequences=True))\n",
        "model4.add(CuDNNGRU(64, return_sequences=True))\n",
        "\n",
        "model4.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n",
        "model4.summary()\n",
        "  \n",
        "model4.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')#, metrics = ['accuracy'])\n",
        "h = model4.fit(X_train_tfidf, Y_train, epochs=5, batch_size=256, verbose= 1, validation_data=(X_valid, Y_valid), sample_weight = sample_weight) \n",
        "desempeño= h.history[\"val_loss\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd48-LGsYzkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  EMBEDDING_DIM = 100\n",
        "  learning_rate = 1e-3\n",
        "  model1 = Sequential()\n",
        "  model1.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_length))\n",
        "  model1.add(CuDNNGRU(64, return_sequences=True))\n",
        "  model1.add(CuDNNGRU(128, return_sequences=False))\n",
        "\n",
        "\n",
        "  model1.add(RepeatVector(max_out_length)) #conection\n",
        "\n",
        "  model1.add(CuDNNGRU(128, return_sequences=True))\n",
        "  model1.add(CuDNNGRU(64, return_sequences=True))\n",
        "\n",
        "  model1.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n",
        "  model1.summary()\n",
        "  \n",
        "  model1.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')#, metrics = ['accuracy'])\n",
        "  h = model1.fit(X_train, Y_train, epochs=mejor_epoch, batch_size=256, verbose= 1, validation_data=(X_valid, Y_valid), sample_weight = sample_weight) \n",
        "  desempeño= h.history[\"val_loss\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKlejw0tYzkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLqlqzdOYzkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh5bD9bcYzkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://stackoverflow.com/questions/53267881/implementing-conv-layers-in-lstm-network\n",
        "    \n",
        "https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
        "    \n",
        "http://mt-class.org/jhu/assets/nmt-book.pdf\n",
        "    \n",
        "https://www.lionbridge.com/blog/translation-localization/neural-machine-translation-artificial-intelligence-works-multilingual-communication/\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9HjFIwSYzkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}